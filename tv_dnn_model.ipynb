{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ## GPU\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "#cpu-gpu configuration\n",
    "#gpu_options = tf.GPUOptions(visible_device_list=\"5,6\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU':2, 'CPU':4}) #max no of GPUs = 1, CPUs =4\n",
    "#config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "\n",
    "#config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sde/jagadish/Anaconda3/envs/jag_env/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import ast \n",
    "import joblib\n",
    "import math\n",
    "import time\n",
    "current_t = time.time()\n",
    "from pandas import DataFrame\n",
    "from array import array\n",
    "import xgboost \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import math\n",
    "import sklearn\n",
    "from pandas import DataFrame\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import pyodbc\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from termcolor import colored\n",
    "from sklearn.metrics import classification_report\n",
    "from multiprocessing import Pool\n",
    "from timeit import default_timer as timer\n",
    "from math import sqrt\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") #Needed to save figures\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics\n",
    "import json\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize, CascadeClassifier\n",
    "import glob\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import time, sys\n",
    "from tkinter import font\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import keras models for Neural Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Input, Activation, add, Dense, Flatten, Dropout, Multiply, Embedding, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D,PReLU\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import theano\n",
    "from keras.layers import Dense, Convolution2D, UpSampling2D, MaxPooling2D, ZeroPadding2D, Flatten, Dropout, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.utils import Sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import LSTM, Dense, Input, Masking, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training data files and append to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame()\n",
    "path = r'/mnt/sde/jagadish/userdata/dl_project/tv_train_files/' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    dd = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(dd)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=df.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = df.drop(columns=df.columns[0])\n",
    "df = df.drop(columns='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>pd_0</th>\n",
       "      <th>pd_1</th>\n",
       "      <th>pd_2</th>\n",
       "      <th>pd_3</th>\n",
       "      <th>pd_4</th>\n",
       "      <th>pd_5</th>\n",
       "      <th>pd_6</th>\n",
       "      <th>pd_7</th>\n",
       "      <th>pd_8</th>\n",
       "      <th>...</th>\n",
       "      <th>hrd_200</th>\n",
       "      <th>hrd_201</th>\n",
       "      <th>hrd_202</th>\n",
       "      <th>hrd_203</th>\n",
       "      <th>hrd_204</th>\n",
       "      <th>hrd_205</th>\n",
       "      <th>hrd_206</th>\n",
       "      <th>hrd_207</th>\n",
       "      <th>hrd_208</th>\n",
       "      <th>hrd_209</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>153.241352</td>\n",
       "      <td>195.438511</td>\n",
       "      <td>350.214416</td>\n",
       "      <td>397.496091</td>\n",
       "      <td>193.081867</td>\n",
       "      <td>341.508392</td>\n",
       "      <td>360.658405</td>\n",
       "      <td>400.570791</td>\n",
       "      <td>393.141593</td>\n",
       "      <td>...</td>\n",
       "      <td>32.683128</td>\n",
       "      <td>20.391135</td>\n",
       "      <td>16.14928</td>\n",
       "      <td>13.527998</td>\n",
       "      <td>19.520154</td>\n",
       "      <td>25.016024</td>\n",
       "      <td>24.775554</td>\n",
       "      <td>6.144659</td>\n",
       "      <td>7.418</td>\n",
       "      <td>2.8512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 3136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y        pd_0        pd_1        pd_2        pd_3        pd_4        pd_5  \\\n",
       "0  1  153.241352  195.438511  350.214416  397.496091  193.081867  341.508392   \n",
       "\n",
       "         pd_6        pd_7        pd_8  ...    hrd_200    hrd_201   hrd_202  \\\n",
       "0  360.658405  400.570791  393.141593  ...  32.683128  20.391135  16.14928   \n",
       "\n",
       "     hrd_203    hrd_204    hrd_205    hrd_206   hrd_207  hrd_208  hrd_209  \n",
       "0  13.527998  19.520154  25.016024  24.775554  6.144659    7.418   2.8512  \n",
       "\n",
       "[1 rows x 3136 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(list(df.filter(regex = 'fd')), axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>pd_0</th>\n",
       "      <th>pd_1</th>\n",
       "      <th>pd_2</th>\n",
       "      <th>pd_3</th>\n",
       "      <th>pd_4</th>\n",
       "      <th>pd_5</th>\n",
       "      <th>pd_6</th>\n",
       "      <th>pd_7</th>\n",
       "      <th>pd_8</th>\n",
       "      <th>...</th>\n",
       "      <th>hrd_200</th>\n",
       "      <th>hrd_201</th>\n",
       "      <th>hrd_202</th>\n",
       "      <th>hrd_203</th>\n",
       "      <th>hrd_204</th>\n",
       "      <th>hrd_205</th>\n",
       "      <th>hrd_206</th>\n",
       "      <th>hrd_207</th>\n",
       "      <th>hrd_208</th>\n",
       "      <th>hrd_209</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>153.241352</td>\n",
       "      <td>195.438511</td>\n",
       "      <td>350.214416</td>\n",
       "      <td>397.496091</td>\n",
       "      <td>193.081867</td>\n",
       "      <td>341.508392</td>\n",
       "      <td>360.658405</td>\n",
       "      <td>400.570791</td>\n",
       "      <td>393.141593</td>\n",
       "      <td>...</td>\n",
       "      <td>32.683128</td>\n",
       "      <td>20.391135</td>\n",
       "      <td>16.14928</td>\n",
       "      <td>13.527998</td>\n",
       "      <td>19.520154</td>\n",
       "      <td>25.016024</td>\n",
       "      <td>24.775554</td>\n",
       "      <td>6.144659</td>\n",
       "      <td>7.418</td>\n",
       "      <td>2.8512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 721 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y        pd_0        pd_1        pd_2        pd_3        pd_4        pd_5  \\\n",
       "0  1  153.241352  195.438511  350.214416  397.496091  193.081867  341.508392   \n",
       "\n",
       "         pd_6        pd_7        pd_8  ...    hrd_200    hrd_201   hrd_202  \\\n",
       "0  360.658405  400.570791  393.141593  ...  32.683128  20.391135  16.14928   \n",
       "\n",
       "     hrd_203    hrd_204    hrd_205    hrd_206   hrd_207  hrd_208  hrd_209  \n",
       "0  13.527998  19.520154  25.016024  24.775554  6.144659    7.418   2.8512  \n",
       "\n",
       "[1 rows x 721 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_table(\"/mnt/sde/jagadish/userdata/dl_project/tv_test_data/tv_8307.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = gf.drop(columns=gf.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.drop(list(gf.filter(regex = 'fd')), axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>pd_0</th>\n",
       "      <th>pd_1</th>\n",
       "      <th>pd_2</th>\n",
       "      <th>pd_3</th>\n",
       "      <th>pd_4</th>\n",
       "      <th>pd_5</th>\n",
       "      <th>pd_6</th>\n",
       "      <th>pd_7</th>\n",
       "      <th>pd_8</th>\n",
       "      <th>...</th>\n",
       "      <th>hrd_200</th>\n",
       "      <th>hrd_201</th>\n",
       "      <th>hrd_202</th>\n",
       "      <th>hrd_203</th>\n",
       "      <th>hrd_204</th>\n",
       "      <th>hrd_205</th>\n",
       "      <th>hrd_206</th>\n",
       "      <th>hrd_207</th>\n",
       "      <th>hrd_208</th>\n",
       "      <th>hrd_209</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>126.655883</td>\n",
       "      <td>169.499292</td>\n",
       "      <td>320.689287</td>\n",
       "      <td>351.885854</td>\n",
       "      <td>160.37001</td>\n",
       "      <td>311.235204</td>\n",
       "      <td>353.641761</td>\n",
       "      <td>380.183866</td>\n",
       "      <td>375.669492</td>\n",
       "      <td>...</td>\n",
       "      <td>42.465596</td>\n",
       "      <td>25.333625</td>\n",
       "      <td>16.816027</td>\n",
       "      <td>10.877662</td>\n",
       "      <td>18.005547</td>\n",
       "      <td>27.541244</td>\n",
       "      <td>35.963244</td>\n",
       "      <td>9.539451</td>\n",
       "      <td>17.962015</td>\n",
       "      <td>8.422704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 721 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y        pd_0        pd_1        pd_2        pd_3       pd_4        pd_5  \\\n",
       "0  1  126.655883  169.499292  320.689287  351.885854  160.37001  311.235204   \n",
       "\n",
       "         pd_6        pd_7        pd_8  ...    hrd_200    hrd_201    hrd_202  \\\n",
       "0  353.641761  380.183866  375.669492  ...  42.465596  25.333625  16.816027   \n",
       "\n",
       "     hrd_203    hrd_204    hrd_205    hrd_206   hrd_207    hrd_208   hrd_209  \n",
       "0  10.877662  18.005547  27.541244  35.963244  9.539451  17.962015  8.422704  \n",
       "\n",
       "[1 rows x 721 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_label(data):\n",
    "    # remove outliers\n",
    "    #data_after = data[(data['price']<400) & (data['price']>1)]\n",
    "    #data_after = data[data['price']>1]\n",
    "    # split features and labels\n",
    "    #train_features = data.drop(['responded'],axis=1)\n",
    "    train_features = data.drop(['Y'],axis=1)\n",
    "    train_labels = data.Y\n",
    "    return train_features,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features,train_labels=get_feature_label(df)\n",
    "train_features=train_features\n",
    "train_labels=train_labels\n",
    "test_features,test_labels=get_feature_label(gf)\n",
    "test_features=test_features\n",
    "test_labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pd_0</th>\n",
       "      <th>pd_1</th>\n",
       "      <th>pd_2</th>\n",
       "      <th>pd_3</th>\n",
       "      <th>pd_4</th>\n",
       "      <th>pd_5</th>\n",
       "      <th>pd_6</th>\n",
       "      <th>pd_7</th>\n",
       "      <th>pd_8</th>\n",
       "      <th>pd_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hrd_200</th>\n",
       "      <th>hrd_201</th>\n",
       "      <th>hrd_202</th>\n",
       "      <th>hrd_203</th>\n",
       "      <th>hrd_204</th>\n",
       "      <th>hrd_205</th>\n",
       "      <th>hrd_206</th>\n",
       "      <th>hrd_207</th>\n",
       "      <th>hrd_208</th>\n",
       "      <th>hrd_209</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153.241352</td>\n",
       "      <td>195.438511</td>\n",
       "      <td>350.214416</td>\n",
       "      <td>397.496091</td>\n",
       "      <td>193.081867</td>\n",
       "      <td>341.508392</td>\n",
       "      <td>360.658405</td>\n",
       "      <td>400.570791</td>\n",
       "      <td>393.141593</td>\n",
       "      <td>498.494538</td>\n",
       "      <td>...</td>\n",
       "      <td>32.683128</td>\n",
       "      <td>20.391135</td>\n",
       "      <td>16.14928</td>\n",
       "      <td>13.527998</td>\n",
       "      <td>19.520154</td>\n",
       "      <td>25.016024</td>\n",
       "      <td>24.775554</td>\n",
       "      <td>6.144659</td>\n",
       "      <td>7.418</td>\n",
       "      <td>2.8512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pd_0        pd_1        pd_2        pd_3        pd_4        pd_5  \\\n",
       "0  153.241352  195.438511  350.214416  397.496091  193.081867  341.508392   \n",
       "\n",
       "         pd_6        pd_7        pd_8        pd_9  ...    hrd_200    hrd_201  \\\n",
       "0  360.658405  400.570791  393.141593  498.494538  ...  32.683128  20.391135   \n",
       "\n",
       "    hrd_202    hrd_203    hrd_204    hrd_205    hrd_206   hrd_207  hrd_208  \\\n",
       "0  16.14928  13.527998  19.520154  25.016024  24.775554  6.144659    7.418   \n",
       "\n",
       "   hrd_209  \n",
       "0   2.8512  \n",
       "\n",
       "[1 rows x 720 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pd_0</th>\n",
       "      <th>pd_1</th>\n",
       "      <th>pd_2</th>\n",
       "      <th>pd_3</th>\n",
       "      <th>pd_4</th>\n",
       "      <th>pd_5</th>\n",
       "      <th>pd_6</th>\n",
       "      <th>pd_7</th>\n",
       "      <th>pd_8</th>\n",
       "      <th>pd_9</th>\n",
       "      <th>...</th>\n",
       "      <th>hrd_200</th>\n",
       "      <th>hrd_201</th>\n",
       "      <th>hrd_202</th>\n",
       "      <th>hrd_203</th>\n",
       "      <th>hrd_204</th>\n",
       "      <th>hrd_205</th>\n",
       "      <th>hrd_206</th>\n",
       "      <th>hrd_207</th>\n",
       "      <th>hrd_208</th>\n",
       "      <th>hrd_209</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.655883</td>\n",
       "      <td>169.499292</td>\n",
       "      <td>320.689287</td>\n",
       "      <td>351.885854</td>\n",
       "      <td>160.37001</td>\n",
       "      <td>311.235204</td>\n",
       "      <td>353.641761</td>\n",
       "      <td>380.183866</td>\n",
       "      <td>375.669492</td>\n",
       "      <td>598.04941</td>\n",
       "      <td>...</td>\n",
       "      <td>42.465596</td>\n",
       "      <td>25.333625</td>\n",
       "      <td>16.816027</td>\n",
       "      <td>10.877662</td>\n",
       "      <td>18.005547</td>\n",
       "      <td>27.541244</td>\n",
       "      <td>35.963244</td>\n",
       "      <td>9.539451</td>\n",
       "      <td>17.962015</td>\n",
       "      <td>8.422704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pd_0        pd_1        pd_2        pd_3       pd_4        pd_5  \\\n",
       "0  126.655883  169.499292  320.689287  351.885854  160.37001  311.235204   \n",
       "\n",
       "         pd_6        pd_7        pd_8       pd_9  ...    hrd_200    hrd_201  \\\n",
       "0  353.641761  380.183866  375.669492  598.04941  ...  42.465596  25.333625   \n",
       "\n",
       "     hrd_202    hrd_203    hrd_204    hrd_205    hrd_206   hrd_207    hrd_208  \\\n",
       "0  16.816027  10.877662  18.005547  27.541244  35.963244  9.539451  17.962015   \n",
       "\n",
       "    hrd_209  \n",
       "0  8.422704  \n",
       "\n",
       "[1 rows x 720 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_features\n",
    "y = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = scaler.transform(test_features)\n",
    "test_features = pd.DataFrame(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.001)\n",
    "adam = optimizers.Adam(lr=0.0001)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "ada =optimizers.Adadelta(lr=0.0001, rho = 0.95, epsilon = 1e-07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               144200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 164,401\n",
      "Trainable params: 164,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13276 samples, validate on 1476 samples\n",
      "Epoch 1/1200\n",
      "13276/13276 [==============================] - 10s 736us/step - loss: 0.3991 - acc: 0.9076 - val_loss: 0.6139 - val_acc: 0.8178\n",
      "Epoch 2/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.2386 - acc: 0.9507 - val_loss: 0.6575 - val_acc: 0.7527\n",
      "Epoch 3/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.2061 - acc: 0.9539 - val_loss: 0.8351 - val_acc: 0.6870\n",
      "Epoch 4/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.1778 - acc: 0.9612 - val_loss: 0.3949 - val_acc: 0.8841\n",
      "Epoch 5/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.1595 - acc: 0.9623 - val_loss: 0.3268 - val_acc: 0.8930\n",
      "Epoch 6/1200\n",
      "13276/13276 [==============================] - 5s 371us/step - loss: 0.1439 - acc: 0.9694 - val_loss: 0.4224 - val_acc: 0.8733\n",
      "Epoch 7/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.1366 - acc: 0.9702 - val_loss: 0.2890 - val_acc: 0.9167\n",
      "Epoch 8/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.1215 - acc: 0.9739 - val_loss: 0.3417 - val_acc: 0.8869\n",
      "Epoch 9/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.1160 - acc: 0.9747 - val_loss: 0.2635 - val_acc: 0.9289\n",
      "Epoch 10/1200\n",
      "13276/13276 [==============================] - 5s 362us/step - loss: 0.1160 - acc: 0.9726 - val_loss: 0.2951 - val_acc: 0.9167\n",
      "Epoch 11/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.1098 - acc: 0.9754 - val_loss: 0.3979 - val_acc: 0.8713\n",
      "Epoch 12/1200\n",
      "13276/13276 [==============================] - 5s 373us/step - loss: 0.1159 - acc: 0.9745 - val_loss: 0.5077 - val_acc: 0.8469\n",
      "Epoch 13/1200\n",
      "13276/13276 [==============================] - 5s 382us/step - loss: 0.1060 - acc: 0.9786 - val_loss: 0.3489 - val_acc: 0.8950\n",
      "Epoch 14/1200\n",
      "13276/13276 [==============================] - 5s 375us/step - loss: 0.1188 - acc: 0.9733 - val_loss: 0.2813 - val_acc: 0.9241\n",
      "Epoch 15/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.1029 - acc: 0.9779 - val_loss: 0.3009 - val_acc: 0.9221\n",
      "Epoch 16/1200\n",
      "13276/13276 [==============================] - 5s 375us/step - loss: 0.1000 - acc: 0.9782 - val_loss: 0.2478 - val_acc: 0.9350\n",
      "Epoch 17/1200\n",
      "13276/13276 [==============================] - 5s 375us/step - loss: 0.0972 - acc: 0.9792 - val_loss: 0.2574 - val_acc: 0.9356\n",
      "Epoch 18/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.1026 - acc: 0.9786 - val_loss: 0.2947 - val_acc: 0.9289\n",
      "Epoch 19/1200\n",
      "13276/13276 [==============================] - 5s 371us/step - loss: 0.1013 - acc: 0.9779 - val_loss: 0.4625 - val_acc: 0.8557\n",
      "Epoch 20/1200\n",
      "13276/13276 [==============================] - 5s 367us/step - loss: 0.0954 - acc: 0.9794 - val_loss: 0.2340 - val_acc: 0.9472\n",
      "Epoch 21/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0917 - acc: 0.9803 - val_loss: 0.2249 - val_acc: 0.9458\n",
      "Epoch 22/1200\n",
      "13276/13276 [==============================] - 5s 374us/step - loss: 0.0931 - acc: 0.9798 - val_loss: 0.2499 - val_acc: 0.9356\n",
      "Epoch 23/1200\n",
      "13276/13276 [==============================] - 5s 367us/step - loss: 0.0939 - acc: 0.9802 - val_loss: 0.3031 - val_acc: 0.9228\n",
      "Epoch 24/1200\n",
      "13276/13276 [==============================] - 5s 375us/step - loss: 0.0967 - acc: 0.9785 - val_loss: 0.2863 - val_acc: 0.9153\n",
      "Epoch 25/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0919 - acc: 0.9800 - val_loss: 0.2792 - val_acc: 0.9248\n",
      "Epoch 26/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0909 - acc: 0.9807 - val_loss: 0.2846 - val_acc: 0.9295\n",
      "Epoch 27/1200\n",
      "13276/13276 [==============================] - 5s 364us/step - loss: 0.0894 - acc: 0.9821 - val_loss: 0.4020 - val_acc: 0.8936\n",
      "Epoch 28/1200\n",
      "13276/13276 [==============================] - 5s 371us/step - loss: 0.0965 - acc: 0.9783 - val_loss: 0.1947 - val_acc: 0.9553\n",
      "Epoch 29/1200\n",
      "13276/13276 [==============================] - 5s 379us/step - loss: 0.0908 - acc: 0.9805 - val_loss: 0.2938 - val_acc: 0.9228\n",
      "Epoch 30/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0907 - acc: 0.9813 - val_loss: 0.2337 - val_acc: 0.9417\n",
      "Epoch 31/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0916 - acc: 0.9802 - val_loss: 0.3136 - val_acc: 0.9140\n",
      "Epoch 32/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0887 - acc: 0.9827 - val_loss: 0.3475 - val_acc: 0.9072\n",
      "Epoch 33/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0962 - acc: 0.9791 - val_loss: 0.2562 - val_acc: 0.9343\n",
      "Epoch 34/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0893 - acc: 0.9812 - val_loss: 0.3096 - val_acc: 0.9289\n",
      "Epoch 35/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0894 - acc: 0.9810 - val_loss: 0.2590 - val_acc: 0.9336\n",
      "Epoch 36/1200\n",
      "13276/13276 [==============================] - 4s 311us/step - loss: 0.0908 - acc: 0.9802 - val_loss: 0.3644 - val_acc: 0.9092\n",
      "Epoch 37/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0832 - acc: 0.9829 - val_loss: 0.2263 - val_acc: 0.9499\n",
      "Epoch 38/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0858 - acc: 0.9821 - val_loss: 0.2943 - val_acc: 0.9228\n",
      "Epoch 39/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0856 - acc: 0.9807 - val_loss: 0.2298 - val_acc: 0.9465\n",
      "Epoch 40/1200\n",
      "13276/13276 [==============================] - 5s 372us/step - loss: 0.0859 - acc: 0.9821 - val_loss: 0.2136 - val_acc: 0.9533\n",
      "Epoch 41/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0836 - acc: 0.9826 - val_loss: 0.2925 - val_acc: 0.9383\n",
      "Epoch 42/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0861 - acc: 0.9820 - val_loss: 0.2740 - val_acc: 0.9356\n",
      "Epoch 43/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0844 - acc: 0.9823 - val_loss: 0.3459 - val_acc: 0.9072\n",
      "Epoch 44/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.0847 - acc: 0.9827 - val_loss: 0.2729 - val_acc: 0.9248\n",
      "Epoch 45/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0885 - acc: 0.9813 - val_loss: 0.2442 - val_acc: 0.9417\n",
      "Epoch 46/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0823 - acc: 0.9840 - val_loss: 0.2307 - val_acc: 0.9424\n",
      "Epoch 47/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0903 - acc: 0.9797 - val_loss: 0.2936 - val_acc: 0.9133\n",
      "Epoch 48/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0814 - acc: 0.9838 - val_loss: 0.2712 - val_acc: 0.9356\n",
      "Epoch 49/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0900 - acc: 0.9806 - val_loss: 0.2396 - val_acc: 0.9478\n",
      "Epoch 50/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0833 - acc: 0.9827 - val_loss: 0.2231 - val_acc: 0.9472\n",
      "Epoch 51/1200\n",
      "13276/13276 [==============================] - 5s 370us/step - loss: 0.0834 - acc: 0.9828 - val_loss: 0.2600 - val_acc: 0.9390\n",
      "Epoch 52/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0844 - acc: 0.9821 - val_loss: 0.4312 - val_acc: 0.8889\n",
      "Epoch 53/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0851 - acc: 0.9821 - val_loss: 0.2330 - val_acc: 0.9438\n",
      "Epoch 54/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0803 - acc: 0.9843 - val_loss: 0.3964 - val_acc: 0.8950\n",
      "Epoch 55/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0857 - acc: 0.9812 - val_loss: 0.2126 - val_acc: 0.9539\n",
      "Epoch 56/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0834 - acc: 0.9831 - val_loss: 0.2155 - val_acc: 0.9526\n",
      "Epoch 57/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0830 - acc: 0.9824 - val_loss: 0.2337 - val_acc: 0.9499\n",
      "Epoch 58/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0825 - acc: 0.9836 - val_loss: 0.2616 - val_acc: 0.9438\n",
      "Epoch 59/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.1305 - acc: 0.9693 - val_loss: 0.3223 - val_acc: 0.9058\n",
      "Epoch 60/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.1111 - acc: 0.9761 - val_loss: 0.2855 - val_acc: 0.9255\n",
      "Epoch 61/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.1091 - acc: 0.9763 - val_loss: 0.2412 - val_acc: 0.9431\n",
      "Epoch 62/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.1042 - acc: 0.9778 - val_loss: 0.2471 - val_acc: 0.9336\n",
      "Epoch 63/1200\n",
      "13276/13276 [==============================] - 4s 292us/step - loss: 0.1007 - acc: 0.9782 - val_loss: 0.2524 - val_acc: 0.9404\n",
      "Epoch 64/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0951 - acc: 0.9800 - val_loss: 0.2243 - val_acc: 0.9451\n",
      "Epoch 65/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.1017 - acc: 0.9781 - val_loss: 0.3038 - val_acc: 0.9234\n",
      "Epoch 66/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0931 - acc: 0.9797 - val_loss: 0.2664 - val_acc: 0.9377\n",
      "Epoch 67/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0917 - acc: 0.9796 - val_loss: 0.2451 - val_acc: 0.9465\n",
      "Epoch 68/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0878 - acc: 0.9819 - val_loss: 0.2249 - val_acc: 0.9492\n",
      "Epoch 69/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0951 - acc: 0.9790 - val_loss: 0.2829 - val_acc: 0.9275\n",
      "Epoch 70/1200\n",
      "13276/13276 [==============================] - 5s 362us/step - loss: 0.0963 - acc: 0.9788 - val_loss: 0.2163 - val_acc: 0.9492\n",
      "Epoch 71/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0981 - acc: 0.9775 - val_loss: 0.2679 - val_acc: 0.9363\n",
      "Epoch 72/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0895 - acc: 0.9815 - val_loss: 0.2588 - val_acc: 0.9377\n",
      "Epoch 73/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0940 - acc: 0.9799 - val_loss: 0.2889 - val_acc: 0.9275\n",
      "Epoch 74/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0858 - acc: 0.9824 - val_loss: 0.2310 - val_acc: 0.9438\n",
      "Epoch 75/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0923 - acc: 0.9783 - val_loss: 0.2196 - val_acc: 0.9458\n",
      "Epoch 76/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0876 - acc: 0.9818 - val_loss: 0.2747 - val_acc: 0.9309\n",
      "Epoch 77/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0931 - acc: 0.9795 - val_loss: 0.2825 - val_acc: 0.9316\n",
      "Epoch 78/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0922 - acc: 0.9799 - val_loss: 0.2483 - val_acc: 0.9438\n",
      "Epoch 79/1200\n",
      "13276/13276 [==============================] - 4s 312us/step - loss: 0.0873 - acc: 0.9824 - val_loss: 0.2271 - val_acc: 0.9411\n",
      "Epoch 80/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0923 - acc: 0.9803 - val_loss: 0.2624 - val_acc: 0.9336\n",
      "Epoch 81/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0882 - acc: 0.9814 - val_loss: 0.2340 - val_acc: 0.9465\n",
      "Epoch 82/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0932 - acc: 0.9798 - val_loss: 0.2436 - val_acc: 0.9431\n",
      "Epoch 83/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0853 - acc: 0.9830 - val_loss: 0.2041 - val_acc: 0.9566\n",
      "Epoch 84/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0975 - acc: 0.9770 - val_loss: 0.2275 - val_acc: 0.9478\n",
      "Epoch 85/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.0866 - acc: 0.9829 - val_loss: 0.3703 - val_acc: 0.8950\n",
      "Epoch 86/1200\n",
      "13276/13276 [==============================] - 5s 367us/step - loss: 0.0917 - acc: 0.9802 - val_loss: 0.2826 - val_acc: 0.9350\n",
      "Epoch 87/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.1033 - acc: 0.9770 - val_loss: 0.2844 - val_acc: 0.9187\n",
      "Epoch 88/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0937 - acc: 0.9803 - val_loss: 0.2443 - val_acc: 0.9478\n",
      "Epoch 89/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.0896 - acc: 0.9813 - val_loss: 0.2104 - val_acc: 0.9472\n",
      "Epoch 90/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0897 - acc: 0.9802 - val_loss: 0.2251 - val_acc: 0.9505\n",
      "Epoch 91/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0868 - acc: 0.9818 - val_loss: 0.2444 - val_acc: 0.9499\n",
      "Epoch 92/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0909 - acc: 0.9807 - val_loss: 0.2542 - val_acc: 0.9356\n",
      "Epoch 93/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0883 - acc: 0.9816 - val_loss: 0.2438 - val_acc: 0.9404\n",
      "Epoch 94/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0895 - acc: 0.9814 - val_loss: 0.3564 - val_acc: 0.8997\n",
      "Epoch 95/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0898 - acc: 0.9809 - val_loss: 0.2739 - val_acc: 0.9350\n",
      "Epoch 96/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0908 - acc: 0.9815 - val_loss: 0.2450 - val_acc: 0.9478\n",
      "Epoch 97/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0921 - acc: 0.9803 - val_loss: 0.2628 - val_acc: 0.9404\n",
      "Epoch 98/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0888 - acc: 0.9807 - val_loss: 0.2248 - val_acc: 0.9519\n",
      "Epoch 99/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0873 - acc: 0.9822 - val_loss: 0.3698 - val_acc: 0.8943\n",
      "Epoch 100/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0873 - acc: 0.9825 - val_loss: 0.2964 - val_acc: 0.9241\n",
      "Epoch 101/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0877 - acc: 0.9815 - val_loss: 0.2428 - val_acc: 0.9431\n",
      "Epoch 102/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0960 - acc: 0.9793 - val_loss: 0.2532 - val_acc: 0.9390\n",
      "Epoch 103/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0870 - acc: 0.9822 - val_loss: 0.2376 - val_acc: 0.9472\n",
      "Epoch 104/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0911 - acc: 0.9806 - val_loss: 0.2215 - val_acc: 0.9526\n",
      "Epoch 105/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0898 - acc: 0.9809 - val_loss: 0.2472 - val_acc: 0.9431\n",
      "Epoch 106/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.0878 - acc: 0.9815 - val_loss: 0.2361 - val_acc: 0.9472\n",
      "Epoch 107/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0863 - acc: 0.9821 - val_loss: 0.2418 - val_acc: 0.9458\n",
      "Epoch 108/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0919 - acc: 0.9794 - val_loss: 0.2568 - val_acc: 0.9390\n",
      "Epoch 109/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0840 - acc: 0.9829 - val_loss: 0.2420 - val_acc: 0.9472\n",
      "Epoch 110/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0890 - acc: 0.9807 - val_loss: 0.2909 - val_acc: 0.9214\n",
      "Epoch 111/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0917 - acc: 0.9805 - val_loss: 0.2363 - val_acc: 0.9465\n",
      "Epoch 112/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0877 - acc: 0.9823 - val_loss: 0.2329 - val_acc: 0.9499\n",
      "Epoch 113/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0924 - acc: 0.9793 - val_loss: 0.2456 - val_acc: 0.9444\n",
      "Epoch 114/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0853 - acc: 0.9832 - val_loss: 0.2540 - val_acc: 0.9397\n",
      "Epoch 115/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0921 - acc: 0.9797 - val_loss: 0.2844 - val_acc: 0.9302\n",
      "Epoch 116/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0871 - acc: 0.9821 - val_loss: 0.3516 - val_acc: 0.9038\n",
      "Epoch 117/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0877 - acc: 0.9822 - val_loss: 0.2734 - val_acc: 0.9322\n",
      "Epoch 118/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0896 - acc: 0.9807 - val_loss: 0.2347 - val_acc: 0.9492\n",
      "Epoch 119/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0840 - acc: 0.9831 - val_loss: 0.2790 - val_acc: 0.9302\n",
      "Epoch 120/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0865 - acc: 0.9819 - val_loss: 0.2332 - val_acc: 0.9438\n",
      "Epoch 121/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0848 - acc: 0.9824 - val_loss: 0.2264 - val_acc: 0.9485\n",
      "Epoch 122/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0892 - acc: 0.9823 - val_loss: 0.3384 - val_acc: 0.9207\n",
      "Epoch 123/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0940 - acc: 0.9799 - val_loss: 0.5340 - val_acc: 0.8279\n",
      "Epoch 124/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0921 - acc: 0.9800 - val_loss: 0.2371 - val_acc: 0.9356\n",
      "Epoch 125/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0870 - acc: 0.9827 - val_loss: 0.2502 - val_acc: 0.9451\n",
      "Epoch 126/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0883 - acc: 0.9813 - val_loss: 0.2785 - val_acc: 0.9343\n",
      "Epoch 127/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0936 - acc: 0.9800 - val_loss: 0.2362 - val_acc: 0.9505\n",
      "Epoch 128/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0877 - acc: 0.9819 - val_loss: 0.2349 - val_acc: 0.9492\n",
      "Epoch 129/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0870 - acc: 0.9824 - val_loss: 0.2341 - val_acc: 0.9465\n",
      "Epoch 130/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0913 - acc: 0.9806 - val_loss: 0.2715 - val_acc: 0.9248\n",
      "Epoch 131/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0903 - acc: 0.9808 - val_loss: 0.2618 - val_acc: 0.9363\n",
      "Epoch 132/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0873 - acc: 0.9824 - val_loss: 0.2814 - val_acc: 0.9302\n",
      "Epoch 133/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0880 - acc: 0.9812 - val_loss: 0.2083 - val_acc: 0.9566\n",
      "Epoch 134/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0880 - acc: 0.9821 - val_loss: 0.2342 - val_acc: 0.9377\n",
      "Epoch 135/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0899 - acc: 0.9803 - val_loss: 0.2545 - val_acc: 0.9411\n",
      "Epoch 136/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0868 - acc: 0.9828 - val_loss: 0.2393 - val_acc: 0.9444\n",
      "Epoch 137/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0868 - acc: 0.9822 - val_loss: 0.2274 - val_acc: 0.9485\n",
      "Epoch 138/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0845 - acc: 0.9834 - val_loss: 0.2539 - val_acc: 0.9383\n",
      "Epoch 139/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0913 - acc: 0.9810 - val_loss: 0.2520 - val_acc: 0.9397\n",
      "Epoch 140/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0833 - acc: 0.9837 - val_loss: 0.2954 - val_acc: 0.9255\n",
      "Epoch 141/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0887 - acc: 0.9815 - val_loss: 0.2208 - val_acc: 0.9533\n",
      "Epoch 142/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0830 - acc: 0.9828 - val_loss: 0.2419 - val_acc: 0.9458\n",
      "Epoch 143/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0816 - acc: 0.9847 - val_loss: 0.2297 - val_acc: 0.9478\n",
      "Epoch 144/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0858 - acc: 0.9820 - val_loss: 0.3889 - val_acc: 0.8991\n",
      "Epoch 145/1200\n",
      "13276/13276 [==============================] - 4s 301us/step - loss: 0.0860 - acc: 0.9822 - val_loss: 0.2256 - val_acc: 0.9492\n",
      "Epoch 146/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0897 - acc: 0.9806 - val_loss: 0.2620 - val_acc: 0.9370\n",
      "Epoch 147/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0817 - acc: 0.9840 - val_loss: 0.2369 - val_acc: 0.9492\n",
      "Epoch 148/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0881 - acc: 0.9812 - val_loss: 0.2637 - val_acc: 0.9424\n",
      "Epoch 149/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0879 - acc: 0.9803 - val_loss: 0.2394 - val_acc: 0.9465\n",
      "Epoch 150/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0794 - acc: 0.9849 - val_loss: 0.2667 - val_acc: 0.9377\n",
      "Epoch 151/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0938 - acc: 0.9790 - val_loss: 0.2659 - val_acc: 0.9404\n",
      "Epoch 152/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0874 - acc: 0.9804 - val_loss: 0.2486 - val_acc: 0.9417\n",
      "Epoch 153/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0835 - acc: 0.9844 - val_loss: 0.2524 - val_acc: 0.9458\n",
      "Epoch 154/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0890 - acc: 0.9811 - val_loss: 0.2485 - val_acc: 0.9465\n",
      "Epoch 155/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0858 - acc: 0.9815 - val_loss: 0.3010 - val_acc: 0.9207\n",
      "Epoch 156/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0907 - acc: 0.9806 - val_loss: 0.2473 - val_acc: 0.9417\n",
      "Epoch 157/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0865 - acc: 0.9822 - val_loss: 0.2324 - val_acc: 0.9499\n",
      "Epoch 158/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0857 - acc: 0.9813 - val_loss: 0.2472 - val_acc: 0.9431\n",
      "Epoch 159/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0923 - acc: 0.9794 - val_loss: 0.2955 - val_acc: 0.9282\n",
      "Epoch 160/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.0880 - acc: 0.9811 - val_loss: 0.3033 - val_acc: 0.9234\n",
      "Epoch 161/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0888 - acc: 0.9813 - val_loss: 0.4526 - val_acc: 0.8652\n",
      "Epoch 162/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0891 - acc: 0.9814 - val_loss: 0.2539 - val_acc: 0.9370\n",
      "Epoch 163/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0901 - acc: 0.9812 - val_loss: 0.2523 - val_acc: 0.9451\n",
      "Epoch 164/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0876 - acc: 0.9805 - val_loss: 0.3089 - val_acc: 0.9228\n",
      "Epoch 165/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0865 - acc: 0.9824 - val_loss: 0.2210 - val_acc: 0.9499\n",
      "Epoch 166/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0837 - acc: 0.9838 - val_loss: 0.2525 - val_acc: 0.9411\n",
      "Epoch 167/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0903 - acc: 0.9802 - val_loss: 0.2221 - val_acc: 0.9499\n",
      "Epoch 168/1200\n",
      "13276/13276 [==============================] - 5s 376us/step - loss: 0.0910 - acc: 0.9810 - val_loss: 0.2462 - val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0858 - acc: 0.9827 - val_loss: 0.2608 - val_acc: 0.9404\n",
      "Epoch 170/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0831 - acc: 0.9834 - val_loss: 0.2348 - val_acc: 0.9485\n",
      "Epoch 171/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0962 - acc: 0.9786 - val_loss: 0.2482 - val_acc: 0.9383\n",
      "Epoch 172/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0865 - acc: 0.9819 - val_loss: 0.2547 - val_acc: 0.9397\n",
      "Epoch 173/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0871 - acc: 0.9819 - val_loss: 0.2413 - val_acc: 0.9438\n",
      "Epoch 174/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0877 - acc: 0.9818 - val_loss: 0.2295 - val_acc: 0.9465\n",
      "Epoch 175/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0804 - acc: 0.9850 - val_loss: 0.2329 - val_acc: 0.9492\n",
      "Epoch 176/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0906 - acc: 0.9800 - val_loss: 0.2767 - val_acc: 0.9343\n",
      "Epoch 177/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0840 - acc: 0.9830 - val_loss: 0.2325 - val_acc: 0.9431\n",
      "Epoch 178/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0892 - acc: 0.9817 - val_loss: 0.2351 - val_acc: 0.9431\n",
      "Epoch 179/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0890 - acc: 0.9813 - val_loss: 0.2349 - val_acc: 0.9451\n",
      "Epoch 180/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0862 - acc: 0.9817 - val_loss: 0.3019 - val_acc: 0.9282\n",
      "Epoch 181/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0923 - acc: 0.9796 - val_loss: 0.2401 - val_acc: 0.9438\n",
      "Epoch 182/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0848 - acc: 0.9820 - val_loss: 0.2254 - val_acc: 0.9485\n",
      "Epoch 183/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0867 - acc: 0.9821 - val_loss: 0.3189 - val_acc: 0.9140\n",
      "Epoch 184/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0890 - acc: 0.9809 - val_loss: 0.2784 - val_acc: 0.9336\n",
      "Epoch 185/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0868 - acc: 0.9812 - val_loss: 0.2598 - val_acc: 0.9390\n",
      "Epoch 186/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0854 - acc: 0.9828 - val_loss: 0.2731 - val_acc: 0.9316\n",
      "Epoch 187/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0886 - acc: 0.9812 - val_loss: 0.2116 - val_acc: 0.9539\n",
      "Epoch 188/1200\n",
      "13276/13276 [==============================] - 5s 373us/step - loss: 0.0870 - acc: 0.9821 - val_loss: 0.2906 - val_acc: 0.9248\n",
      "Epoch 189/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0847 - acc: 0.9821 - val_loss: 0.2422 - val_acc: 0.9444\n",
      "Epoch 190/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0890 - acc: 0.9813 - val_loss: 0.2754 - val_acc: 0.9329\n",
      "Epoch 191/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0892 - acc: 0.9815 - val_loss: 0.2846 - val_acc: 0.9289\n",
      "Epoch 192/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.0890 - acc: 0.9821 - val_loss: 0.6337 - val_acc: 0.8137\n",
      "Epoch 193/1200\n",
      "13276/13276 [==============================] - 5s 368us/step - loss: 0.0874 - acc: 0.9818 - val_loss: 0.2359 - val_acc: 0.9465\n",
      "Epoch 194/1200\n",
      "13276/13276 [==============================] - 4s 289us/step - loss: 0.0861 - acc: 0.9825 - val_loss: 0.2559 - val_acc: 0.9417\n",
      "Epoch 195/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0901 - acc: 0.9816 - val_loss: 0.2258 - val_acc: 0.9472\n",
      "Epoch 196/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0860 - acc: 0.9827 - val_loss: 0.2649 - val_acc: 0.9411\n",
      "Epoch 197/1200\n",
      "13276/13276 [==============================] - 5s 371us/step - loss: 0.0921 - acc: 0.9797 - val_loss: 0.2851 - val_acc: 0.9336\n",
      "Epoch 198/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0922 - acc: 0.9804 - val_loss: 0.2410 - val_acc: 0.9472\n",
      "Epoch 199/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0848 - acc: 0.9831 - val_loss: 0.2505 - val_acc: 0.9431\n",
      "Epoch 200/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0910 - acc: 0.9812 - val_loss: 0.2722 - val_acc: 0.9350\n",
      "Epoch 201/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0842 - acc: 0.9832 - val_loss: 0.2541 - val_acc: 0.9397\n",
      "Epoch 202/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0958 - acc: 0.9783 - val_loss: 0.2449 - val_acc: 0.9411\n",
      "Epoch 203/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.1025 - acc: 0.9772 - val_loss: 0.2576 - val_acc: 0.9404\n",
      "Epoch 204/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0878 - acc: 0.9821 - val_loss: 0.2559 - val_acc: 0.9411\n",
      "Epoch 205/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0813 - acc: 0.9842 - val_loss: 0.2632 - val_acc: 0.9417\n",
      "Epoch 206/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0858 - acc: 0.9820 - val_loss: 0.2506 - val_acc: 0.9397\n",
      "Epoch 207/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0914 - acc: 0.9799 - val_loss: 0.2496 - val_acc: 0.9478\n",
      "Epoch 208/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0836 - acc: 0.9837 - val_loss: 0.2367 - val_acc: 0.9458\n",
      "Epoch 209/1200\n",
      "13276/13276 [==============================] - 4s 312us/step - loss: 0.0869 - acc: 0.9818 - val_loss: 0.4657 - val_acc: 0.8720\n",
      "Epoch 210/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.1077 - acc: 0.9760 - val_loss: 0.2753 - val_acc: 0.9316\n",
      "Epoch 211/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0892 - acc: 0.9818 - val_loss: 0.2404 - val_acc: 0.9472\n",
      "Epoch 212/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0877 - acc: 0.9820 - val_loss: 0.2235 - val_acc: 0.9512\n",
      "Epoch 213/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0896 - acc: 0.9807 - val_loss: 0.2544 - val_acc: 0.9370\n",
      "Epoch 214/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0919 - acc: 0.9794 - val_loss: 0.2404 - val_acc: 0.9431\n",
      "Epoch 215/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0890 - acc: 0.9817 - val_loss: 0.2357 - val_acc: 0.9499\n",
      "Epoch 216/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0840 - acc: 0.9831 - val_loss: 0.2526 - val_acc: 0.9383\n",
      "Epoch 217/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0899 - acc: 0.9811 - val_loss: 0.2585 - val_acc: 0.9404\n",
      "Epoch 218/1200\n",
      "13276/13276 [==============================] - 4s 297us/step - loss: 0.0836 - acc: 0.9837 - val_loss: 0.2269 - val_acc: 0.9492\n",
      "Epoch 219/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0857 - acc: 0.9828 - val_loss: 0.2636 - val_acc: 0.9417\n",
      "Epoch 220/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0923 - acc: 0.9798 - val_loss: 0.2247 - val_acc: 0.9485\n",
      "Epoch 221/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0871 - acc: 0.9828 - val_loss: 0.2542 - val_acc: 0.9377\n",
      "Epoch 222/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0804 - acc: 0.9849 - val_loss: 0.2618 - val_acc: 0.9404\n",
      "Epoch 223/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0929 - acc: 0.9808 - val_loss: 0.2306 - val_acc: 0.9492\n",
      "Epoch 224/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0920 - acc: 0.9803 - val_loss: 0.2266 - val_acc: 0.9458\n",
      "Epoch 225/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0851 - acc: 0.9833 - val_loss: 0.2132 - val_acc: 0.9546\n",
      "Epoch 226/1200\n",
      "13276/13276 [==============================] - 5s 369us/step - loss: 0.0879 - acc: 0.9820 - val_loss: 0.2490 - val_acc: 0.9431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0857 - acc: 0.9828 - val_loss: 0.2246 - val_acc: 0.9512\n",
      "Epoch 228/1200\n",
      "13276/13276 [==============================] - 4s 286us/step - loss: 0.0893 - acc: 0.9816 - val_loss: 0.2823 - val_acc: 0.9302\n",
      "Epoch 229/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0849 - acc: 0.9827 - val_loss: 0.2234 - val_acc: 0.9485\n",
      "Epoch 230/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0867 - acc: 0.9826 - val_loss: 0.2444 - val_acc: 0.9438\n",
      "Epoch 231/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0849 - acc: 0.9831 - val_loss: 0.2340 - val_acc: 0.9478\n",
      "Epoch 232/1200\n",
      "13276/13276 [==============================] - 5s 373us/step - loss: 0.0867 - acc: 0.9828 - val_loss: 0.3820 - val_acc: 0.8936\n",
      "Epoch 233/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0887 - acc: 0.9804 - val_loss: 0.2563 - val_acc: 0.9363\n",
      "Epoch 234/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0895 - acc: 0.9809 - val_loss: 0.2365 - val_acc: 0.9458\n",
      "Epoch 235/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0864 - acc: 0.9831 - val_loss: 0.2287 - val_acc: 0.9499\n",
      "Epoch 236/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0832 - acc: 0.9836 - val_loss: 0.2304 - val_acc: 0.9485\n",
      "Epoch 237/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0867 - acc: 0.9823 - val_loss: 0.2270 - val_acc: 0.9485\n",
      "Epoch 238/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0916 - acc: 0.9806 - val_loss: 0.2426 - val_acc: 0.9458\n",
      "Epoch 239/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0846 - acc: 0.9837 - val_loss: 0.2461 - val_acc: 0.9343\n",
      "Epoch 240/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0841 - acc: 0.9835 - val_loss: 0.2193 - val_acc: 0.9505\n",
      "Epoch 241/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0856 - acc: 0.9819 - val_loss: 0.2624 - val_acc: 0.9363\n",
      "Epoch 242/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0882 - acc: 0.9820 - val_loss: 0.2896 - val_acc: 0.9255\n",
      "Epoch 243/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0864 - acc: 0.9820 - val_loss: 0.2396 - val_acc: 0.9438\n",
      "Epoch 244/1200\n",
      "13276/13276 [==============================] - 5s 378us/step - loss: 0.0896 - acc: 0.9805 - val_loss: 0.2411 - val_acc: 0.9478\n",
      "Epoch 245/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0882 - acc: 0.9818 - val_loss: 0.2255 - val_acc: 0.9499\n",
      "Epoch 246/1200\n",
      "13276/13276 [==============================] - 4s 309us/step - loss: 0.0853 - acc: 0.9828 - val_loss: 0.2177 - val_acc: 0.9505\n",
      "Epoch 247/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0811 - acc: 0.9833 - val_loss: 0.5224 - val_acc: 0.8401\n",
      "Epoch 248/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.1349 - acc: 0.9681 - val_loss: 0.2395 - val_acc: 0.9316\n",
      "Epoch 249/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.1105 - acc: 0.9758 - val_loss: 0.2481 - val_acc: 0.9370\n",
      "Epoch 250/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.1096 - acc: 0.9754 - val_loss: 0.2198 - val_acc: 0.9458\n",
      "Epoch 251/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0918 - acc: 0.9806 - val_loss: 0.2525 - val_acc: 0.9377\n",
      "Epoch 252/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0924 - acc: 0.9800 - val_loss: 0.2391 - val_acc: 0.9451\n",
      "Epoch 253/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0852 - acc: 0.9827 - val_loss: 0.2125 - val_acc: 0.9539\n",
      "Epoch 254/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0915 - acc: 0.9794 - val_loss: 0.2347 - val_acc: 0.9458\n",
      "Epoch 255/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0940 - acc: 0.9794 - val_loss: 0.2458 - val_acc: 0.9424\n",
      "Epoch 256/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0879 - acc: 0.9812 - val_loss: 0.3135 - val_acc: 0.9099\n",
      "Epoch 257/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0909 - acc: 0.9797 - val_loss: 0.2603 - val_acc: 0.9390\n",
      "Epoch 258/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0910 - acc: 0.9801 - val_loss: 0.2419 - val_acc: 0.9411\n",
      "Epoch 259/1200\n",
      "13276/13276 [==============================] - 5s 370us/step - loss: 0.0861 - acc: 0.9818 - val_loss: 0.2507 - val_acc: 0.9363\n",
      "Epoch 260/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0890 - acc: 0.9807 - val_loss: 0.2406 - val_acc: 0.9438\n",
      "Epoch 261/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.0892 - acc: 0.9803 - val_loss: 0.2743 - val_acc: 0.9356\n",
      "Epoch 262/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0850 - acc: 0.9821 - val_loss: 0.2354 - val_acc: 0.9438\n",
      "Epoch 263/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0873 - acc: 0.9816 - val_loss: 0.2231 - val_acc: 0.9478\n",
      "Epoch 264/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0965 - acc: 0.9782 - val_loss: 0.2488 - val_acc: 0.9424\n",
      "Epoch 265/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0867 - acc: 0.9826 - val_loss: 0.2523 - val_acc: 0.9363\n",
      "Epoch 266/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0885 - acc: 0.9809 - val_loss: 0.2333 - val_acc: 0.9438\n",
      "Epoch 267/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0974 - acc: 0.9776 - val_loss: 0.2952 - val_acc: 0.9180\n",
      "Epoch 268/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0891 - acc: 0.9803 - val_loss: 0.2247 - val_acc: 0.9465\n",
      "Epoch 269/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0910 - acc: 0.9806 - val_loss: 0.2494 - val_acc: 0.9444\n",
      "Epoch 270/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0856 - acc: 0.9822 - val_loss: 0.2241 - val_acc: 0.9458\n",
      "Epoch 271/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0903 - acc: 0.9806 - val_loss: 0.2396 - val_acc: 0.9417\n",
      "Epoch 272/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0837 - acc: 0.9827 - val_loss: 0.2238 - val_acc: 0.9499\n",
      "Epoch 273/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0823 - acc: 0.9837 - val_loss: 0.2357 - val_acc: 0.9451\n",
      "Epoch 274/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0897 - acc: 0.9803 - val_loss: 0.2604 - val_acc: 0.9404\n",
      "Epoch 275/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0844 - acc: 0.9828 - val_loss: 0.2891 - val_acc: 0.9350\n",
      "Epoch 276/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.0866 - acc: 0.9819 - val_loss: 0.2606 - val_acc: 0.9397\n",
      "Epoch 277/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0860 - acc: 0.9829 - val_loss: 0.2588 - val_acc: 0.9411\n",
      "Epoch 278/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0912 - acc: 0.9807 - val_loss: 0.2451 - val_acc: 0.9424\n",
      "Epoch 279/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0853 - acc: 0.9817 - val_loss: 0.2523 - val_acc: 0.9417\n",
      "Epoch 280/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0842 - acc: 0.9826 - val_loss: 0.2977 - val_acc: 0.9241\n",
      "Epoch 281/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0889 - acc: 0.9802 - val_loss: 0.2525 - val_acc: 0.9383\n",
      "Epoch 282/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0821 - acc: 0.9834 - val_loss: 0.2645 - val_acc: 0.9370\n",
      "Epoch 283/1200\n",
      "13276/13276 [==============================] - 4s 308us/step - loss: 0.0897 - acc: 0.9801 - val_loss: 0.2268 - val_acc: 0.9451\n",
      "Epoch 284/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0838 - acc: 0.9831 - val_loss: 0.2564 - val_acc: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0855 - acc: 0.9815 - val_loss: 0.2210 - val_acc: 0.9505\n",
      "Epoch 286/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0895 - acc: 0.9816 - val_loss: 0.2199 - val_acc: 0.9472\n",
      "Epoch 287/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0851 - acc: 0.9828 - val_loss: 0.2820 - val_acc: 0.9295\n",
      "Epoch 288/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0891 - acc: 0.9815 - val_loss: 0.2270 - val_acc: 0.9485\n",
      "Epoch 289/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0881 - acc: 0.9809 - val_loss: 0.2050 - val_acc: 0.9560\n",
      "Epoch 290/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0856 - acc: 0.9815 - val_loss: 0.2640 - val_acc: 0.9390\n",
      "Epoch 291/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0869 - acc: 0.9818 - val_loss: 0.2860 - val_acc: 0.9302\n",
      "Epoch 292/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0875 - acc: 0.9819 - val_loss: 0.2405 - val_acc: 0.9472\n",
      "Epoch 293/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0837 - acc: 0.9828 - val_loss: 0.2389 - val_acc: 0.9444\n",
      "Epoch 294/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0851 - acc: 0.9816 - val_loss: 0.2275 - val_acc: 0.9472\n",
      "Epoch 295/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0914 - acc: 0.9803 - val_loss: 0.2557 - val_acc: 0.9363\n",
      "Epoch 296/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0859 - acc: 0.9828 - val_loss: 0.2635 - val_acc: 0.9397\n",
      "Epoch 297/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0843 - acc: 0.9836 - val_loss: 0.2420 - val_acc: 0.9438\n",
      "Epoch 298/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0871 - acc: 0.9809 - val_loss: 0.2391 - val_acc: 0.9472\n",
      "Epoch 299/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0922 - acc: 0.9800 - val_loss: 0.2271 - val_acc: 0.9485\n",
      "Epoch 300/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0908 - acc: 0.9812 - val_loss: 0.2424 - val_acc: 0.9458\n",
      "Epoch 301/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0860 - acc: 0.9825 - val_loss: 0.2333 - val_acc: 0.9492\n",
      "Epoch 302/1200\n",
      "13276/13276 [==============================] - 4s 311us/step - loss: 0.0880 - acc: 0.9807 - val_loss: 0.2354 - val_acc: 0.9485\n",
      "Epoch 303/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0858 - acc: 0.9821 - val_loss: 0.2409 - val_acc: 0.9438\n",
      "Epoch 304/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0878 - acc: 0.9820 - val_loss: 0.2224 - val_acc: 0.9505\n",
      "Epoch 305/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0832 - acc: 0.9836 - val_loss: 0.2140 - val_acc: 0.9539\n",
      "Epoch 306/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0887 - acc: 0.9812 - val_loss: 0.3193 - val_acc: 0.9201\n",
      "Epoch 307/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0824 - acc: 0.9837 - val_loss: 0.2388 - val_acc: 0.9465\n",
      "Epoch 308/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0879 - acc: 0.9818 - val_loss: 0.2380 - val_acc: 0.9451\n",
      "Epoch 309/1200\n",
      "13276/13276 [==============================] - 4s 309us/step - loss: 0.0949 - acc: 0.9786 - val_loss: 0.2275 - val_acc: 0.9512\n",
      "Epoch 310/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0841 - acc: 0.9834 - val_loss: 0.2285 - val_acc: 0.9478\n",
      "Epoch 311/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0846 - acc: 0.9824 - val_loss: 0.2293 - val_acc: 0.9458\n",
      "Epoch 312/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0850 - acc: 0.9828 - val_loss: 0.2393 - val_acc: 0.9458\n",
      "Epoch 313/1200\n",
      "13276/13276 [==============================] - 5s 364us/step - loss: 0.0886 - acc: 0.9801 - val_loss: 0.5199 - val_acc: 0.8482\n",
      "Epoch 314/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0892 - acc: 0.9810 - val_loss: 0.2172 - val_acc: 0.9526\n",
      "Epoch 315/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0867 - acc: 0.9818 - val_loss: 0.2188 - val_acc: 0.9465\n",
      "Epoch 316/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0861 - acc: 0.9830 - val_loss: 0.2247 - val_acc: 0.9472\n",
      "Epoch 317/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0866 - acc: 0.9821 - val_loss: 0.2468 - val_acc: 0.9370\n",
      "Epoch 318/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0881 - acc: 0.9815 - val_loss: 0.2215 - val_acc: 0.9505\n",
      "Epoch 319/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0890 - acc: 0.9817 - val_loss: 0.2338 - val_acc: 0.9465\n",
      "Epoch 320/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0869 - acc: 0.9821 - val_loss: 0.2161 - val_acc: 0.9539\n",
      "Epoch 321/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0864 - acc: 0.9827 - val_loss: 0.2613 - val_acc: 0.9377\n",
      "Epoch 322/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0881 - acc: 0.9818 - val_loss: 0.2303 - val_acc: 0.9492\n",
      "Epoch 323/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0880 - acc: 0.9818 - val_loss: 0.2521 - val_acc: 0.9424\n",
      "Epoch 324/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0850 - acc: 0.9829 - val_loss: 0.2252 - val_acc: 0.9512\n",
      "Epoch 325/1200\n",
      "13276/13276 [==============================] - 5s 373us/step - loss: 0.1095 - acc: 0.9746 - val_loss: 0.2581 - val_acc: 0.9383\n",
      "Epoch 326/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.1122 - acc: 0.9741 - val_loss: 0.2617 - val_acc: 0.9383\n",
      "Epoch 327/1200\n",
      "13276/13276 [==============================] - 4s 295us/step - loss: 0.1118 - acc: 0.9745 - val_loss: 0.2687 - val_acc: 0.9214\n",
      "Epoch 328/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0977 - acc: 0.9784 - val_loss: 0.2538 - val_acc: 0.9377\n",
      "Epoch 329/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0933 - acc: 0.9806 - val_loss: 0.2468 - val_acc: 0.9404\n",
      "Epoch 330/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0873 - acc: 0.9808 - val_loss: 0.3636 - val_acc: 0.8970\n",
      "Epoch 331/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0883 - acc: 0.9815 - val_loss: 0.2358 - val_acc: 0.9438\n",
      "Epoch 332/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0854 - acc: 0.9821 - val_loss: 0.2490 - val_acc: 0.9424\n",
      "Epoch 333/1200\n",
      "13276/13276 [==============================] - 5s 364us/step - loss: 0.0926 - acc: 0.9790 - val_loss: 0.2486 - val_acc: 0.9404\n",
      "Epoch 334/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.0892 - acc: 0.9807 - val_loss: 0.2387 - val_acc: 0.9438\n",
      "Epoch 335/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0849 - acc: 0.9834 - val_loss: 0.2705 - val_acc: 0.9295\n",
      "Epoch 336/1200\n",
      "13276/13276 [==============================] - 5s 362us/step - loss: 0.0910 - acc: 0.9794 - val_loss: 0.2150 - val_acc: 0.9505\n",
      "Epoch 337/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0892 - acc: 0.9806 - val_loss: 0.2410 - val_acc: 0.9438\n",
      "Epoch 338/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0890 - acc: 0.9807 - val_loss: 0.2798 - val_acc: 0.9275\n",
      "Epoch 339/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0880 - acc: 0.9808 - val_loss: 0.2664 - val_acc: 0.9316\n",
      "Epoch 340/1200\n",
      "13276/13276 [==============================] - 5s 362us/step - loss: 0.0850 - acc: 0.9828 - val_loss: 0.3270 - val_acc: 0.9079\n",
      "Epoch 341/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0868 - acc: 0.9813 - val_loss: 0.2619 - val_acc: 0.9377\n",
      "Epoch 342/1200\n",
      "13276/13276 [==============================] - 5s 375us/step - loss: 0.0878 - acc: 0.9814 - val_loss: 0.2441 - val_acc: 0.9390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0883 - acc: 0.9811 - val_loss: 0.2382 - val_acc: 0.9444\n",
      "Epoch 344/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0863 - acc: 0.9821 - val_loss: 0.2450 - val_acc: 0.9397\n",
      "Epoch 345/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0946 - acc: 0.9788 - val_loss: 0.2377 - val_acc: 0.9431\n",
      "Epoch 346/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0854 - acc: 0.9824 - val_loss: 0.2500 - val_acc: 0.9404\n",
      "Epoch 347/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0915 - acc: 0.9813 - val_loss: 0.2571 - val_acc: 0.9438\n",
      "Epoch 348/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0860 - acc: 0.9824 - val_loss: 0.2592 - val_acc: 0.9356\n",
      "Epoch 349/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0850 - acc: 0.9830 - val_loss: 0.2451 - val_acc: 0.9390\n",
      "Epoch 350/1200\n",
      "13276/13276 [==============================] - 4s 298us/step - loss: 0.0827 - acc: 0.9840 - val_loss: 0.2356 - val_acc: 0.9472\n",
      "Epoch 351/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0896 - acc: 0.9811 - val_loss: 0.2290 - val_acc: 0.9472\n",
      "Epoch 352/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0828 - acc: 0.9828 - val_loss: 0.2184 - val_acc: 0.9546\n",
      "Epoch 353/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0878 - acc: 0.9815 - val_loss: 0.2346 - val_acc: 0.9458\n",
      "Epoch 354/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0874 - acc: 0.9812 - val_loss: 0.3157 - val_acc: 0.9153\n",
      "Epoch 355/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0912 - acc: 0.9805 - val_loss: 0.3021 - val_acc: 0.9201\n",
      "Epoch 356/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0855 - acc: 0.9831 - val_loss: 0.2473 - val_acc: 0.9444\n",
      "Epoch 357/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0874 - acc: 0.9818 - val_loss: 0.2401 - val_acc: 0.9404\n",
      "Epoch 358/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0881 - acc: 0.9804 - val_loss: 0.2429 - val_acc: 0.9404\n",
      "Epoch 359/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0832 - acc: 0.9824 - val_loss: 0.2318 - val_acc: 0.9472\n",
      "Epoch 360/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0911 - acc: 0.9803 - val_loss: 0.2674 - val_acc: 0.9329\n",
      "Epoch 361/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0866 - acc: 0.9825 - val_loss: 0.2195 - val_acc: 0.9492\n",
      "Epoch 362/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0875 - acc: 0.9818 - val_loss: 0.2151 - val_acc: 0.9512\n",
      "Epoch 363/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0851 - acc: 0.9825 - val_loss: 0.2207 - val_acc: 0.9505\n",
      "Epoch 364/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0857 - acc: 0.9821 - val_loss: 0.2526 - val_acc: 0.9377\n",
      "Epoch 365/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0871 - acc: 0.9816 - val_loss: 0.2578 - val_acc: 0.9390\n",
      "Epoch 366/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0852 - acc: 0.9831 - val_loss: 0.2367 - val_acc: 0.9472\n",
      "Epoch 367/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0831 - acc: 0.9830 - val_loss: 0.2474 - val_acc: 0.9438\n",
      "Epoch 368/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0873 - acc: 0.9811 - val_loss: 0.2512 - val_acc: 0.9404\n",
      "Epoch 369/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0996 - acc: 0.9777 - val_loss: 0.2456 - val_acc: 0.9390\n",
      "Epoch 370/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.1111 - acc: 0.9748 - val_loss: 0.2342 - val_acc: 0.9424\n",
      "Epoch 371/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.1135 - acc: 0.9733 - val_loss: 0.2952 - val_acc: 0.9173\n",
      "Epoch 372/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.1066 - acc: 0.9753 - val_loss: 0.2638 - val_acc: 0.9289\n",
      "Epoch 373/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0881 - acc: 0.9824 - val_loss: 0.2410 - val_acc: 0.9438\n",
      "Epoch 374/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0907 - acc: 0.9794 - val_loss: 0.3538 - val_acc: 0.8957\n",
      "Epoch 375/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0865 - acc: 0.9815 - val_loss: 0.2356 - val_acc: 0.9472\n",
      "Epoch 376/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0879 - acc: 0.9820 - val_loss: 0.2505 - val_acc: 0.9438\n",
      "Epoch 377/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0872 - acc: 0.9818 - val_loss: 0.2476 - val_acc: 0.9377\n",
      "Epoch 378/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0874 - acc: 0.9815 - val_loss: 0.2423 - val_acc: 0.9417\n",
      "Epoch 379/1200\n",
      "13276/13276 [==============================] - 4s 291us/step - loss: 0.0903 - acc: 0.9810 - val_loss: 0.2227 - val_acc: 0.9492\n",
      "Epoch 380/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0836 - acc: 0.9827 - val_loss: 0.2342 - val_acc: 0.9444\n",
      "Epoch 381/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.1015 - acc: 0.9781 - val_loss: 0.2940 - val_acc: 0.9146\n",
      "Epoch 382/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.1101 - acc: 0.9748 - val_loss: 0.2591 - val_acc: 0.9356\n",
      "Epoch 383/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.1120 - acc: 0.9740 - val_loss: 0.2412 - val_acc: 0.9438\n",
      "Epoch 384/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0995 - acc: 0.9777 - val_loss: 0.2768 - val_acc: 0.9289\n",
      "Epoch 385/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0872 - acc: 0.9811 - val_loss: 0.2423 - val_acc: 0.9444\n",
      "Epoch 386/1200\n",
      "13276/13276 [==============================] - 4s 313us/step - loss: 0.0865 - acc: 0.9821 - val_loss: 0.2698 - val_acc: 0.9322\n",
      "Epoch 387/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0891 - acc: 0.9809 - val_loss: 0.2548 - val_acc: 0.9383\n",
      "Epoch 388/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0901 - acc: 0.9803 - val_loss: 0.2630 - val_acc: 0.9383\n",
      "Epoch 389/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0878 - acc: 0.9809 - val_loss: 0.2288 - val_acc: 0.9512\n",
      "Epoch 390/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0996 - acc: 0.9779 - val_loss: 0.2632 - val_acc: 0.9282\n",
      "Epoch 391/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0898 - acc: 0.9813 - val_loss: 0.2207 - val_acc: 0.9465\n",
      "Epoch 392/1200\n",
      "13276/13276 [==============================] - 4s 289us/step - loss: 0.0903 - acc: 0.9803 - val_loss: 0.2758 - val_acc: 0.9309\n",
      "Epoch 393/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0862 - acc: 0.9816 - val_loss: 0.2563 - val_acc: 0.9377\n",
      "Epoch 394/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0825 - acc: 0.9826 - val_loss: 0.2433 - val_acc: 0.9451\n",
      "Epoch 395/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0867 - acc: 0.9826 - val_loss: 0.2339 - val_acc: 0.9492\n",
      "Epoch 396/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0897 - acc: 0.9808 - val_loss: 0.2334 - val_acc: 0.9458\n",
      "Epoch 397/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0896 - acc: 0.9818 - val_loss: 0.2270 - val_acc: 0.9451\n",
      "Epoch 398/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0860 - acc: 0.9825 - val_loss: 0.2314 - val_acc: 0.9472\n",
      "Epoch 399/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0860 - acc: 0.9812 - val_loss: 0.3055 - val_acc: 0.9268\n",
      "Epoch 400/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0909 - acc: 0.9802 - val_loss: 0.2463 - val_acc: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0836 - acc: 0.9827 - val_loss: 0.2632 - val_acc: 0.9397\n",
      "Epoch 402/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0911 - acc: 0.9806 - val_loss: 0.2553 - val_acc: 0.9397\n",
      "Epoch 403/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0885 - acc: 0.9803 - val_loss: 0.2295 - val_acc: 0.9458\n",
      "Epoch 404/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0880 - acc: 0.9812 - val_loss: 0.2630 - val_acc: 0.9363\n",
      "Epoch 405/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0870 - acc: 0.9815 - val_loss: 0.2252 - val_acc: 0.9458\n",
      "Epoch 406/1200\n",
      "13276/13276 [==============================] - 4s 298us/step - loss: 0.0813 - acc: 0.9843 - val_loss: 0.2625 - val_acc: 0.9397\n",
      "Epoch 407/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0841 - acc: 0.9830 - val_loss: 0.2179 - val_acc: 0.9526\n",
      "Epoch 408/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0879 - acc: 0.9806 - val_loss: 0.2716 - val_acc: 0.9390\n",
      "Epoch 409/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0874 - acc: 0.9809 - val_loss: 0.2491 - val_acc: 0.9397\n",
      "Epoch 410/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0795 - acc: 0.9845 - val_loss: 0.2456 - val_acc: 0.9458\n",
      "Epoch 411/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0860 - acc: 0.9815 - val_loss: 0.2783 - val_acc: 0.9295\n",
      "Epoch 412/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0862 - acc: 0.9817 - val_loss: 0.2355 - val_acc: 0.9458\n",
      "Epoch 413/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.0930 - acc: 0.9797 - val_loss: 0.2735 - val_acc: 0.9350\n",
      "Epoch 414/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0842 - acc: 0.9821 - val_loss: 0.2163 - val_acc: 0.9553\n",
      "Epoch 415/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0824 - acc: 0.9834 - val_loss: 0.2597 - val_acc: 0.9424\n",
      "Epoch 416/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0873 - acc: 0.9812 - val_loss: 0.2494 - val_acc: 0.9465\n",
      "Epoch 417/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0931 - acc: 0.9782 - val_loss: 0.2507 - val_acc: 0.9417\n",
      "Epoch 418/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0856 - acc: 0.9817 - val_loss: 0.2419 - val_acc: 0.9458\n",
      "Epoch 419/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0818 - acc: 0.9829 - val_loss: 0.2535 - val_acc: 0.9431\n",
      "Epoch 420/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0847 - acc: 0.9824 - val_loss: 0.3522 - val_acc: 0.9106\n",
      "Epoch 421/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0906 - acc: 0.9800 - val_loss: 0.2572 - val_acc: 0.9397\n",
      "Epoch 422/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0862 - acc: 0.9823 - val_loss: 0.2310 - val_acc: 0.9485\n",
      "Epoch 423/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0941 - acc: 0.9791 - val_loss: 0.2200 - val_acc: 0.9519\n",
      "Epoch 424/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0861 - acc: 0.9818 - val_loss: 0.2300 - val_acc: 0.9478\n",
      "Epoch 425/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0885 - acc: 0.9815 - val_loss: 0.2587 - val_acc: 0.9383\n",
      "Epoch 426/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0870 - acc: 0.9814 - val_loss: 0.2113 - val_acc: 0.9553\n",
      "Epoch 427/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0870 - acc: 0.9800 - val_loss: 0.2414 - val_acc: 0.9492\n",
      "Epoch 428/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0856 - acc: 0.9824 - val_loss: 0.2229 - val_acc: 0.9512\n",
      "Epoch 429/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0841 - acc: 0.9828 - val_loss: 0.3175 - val_acc: 0.9173\n",
      "Epoch 430/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0862 - acc: 0.9831 - val_loss: 0.3094 - val_acc: 0.9262\n",
      "Epoch 431/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0891 - acc: 0.9803 - val_loss: 0.2408 - val_acc: 0.9411\n",
      "Epoch 432/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0844 - acc: 0.9828 - val_loss: 0.2435 - val_acc: 0.9444\n",
      "Epoch 433/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0896 - acc: 0.9806 - val_loss: 0.2975 - val_acc: 0.9282\n",
      "Epoch 434/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0843 - acc: 0.9825 - val_loss: 0.2361 - val_acc: 0.9485\n",
      "Epoch 435/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0886 - acc: 0.9814 - val_loss: 0.2286 - val_acc: 0.9472\n",
      "Epoch 436/1200\n",
      "13276/13276 [==============================] - 4s 289us/step - loss: 0.0830 - acc: 0.9819 - val_loss: 0.2343 - val_acc: 0.9458\n",
      "Epoch 437/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0897 - acc: 0.9794 - val_loss: 0.2426 - val_acc: 0.9444\n",
      "Epoch 438/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0869 - acc: 0.9812 - val_loss: 0.2214 - val_acc: 0.9533\n",
      "Epoch 439/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0817 - acc: 0.9831 - val_loss: 0.2444 - val_acc: 0.9465\n",
      "Epoch 440/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0869 - acc: 0.9825 - val_loss: 0.2292 - val_acc: 0.9499\n",
      "Epoch 441/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0890 - acc: 0.9807 - val_loss: 0.2348 - val_acc: 0.9492\n",
      "Epoch 442/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0909 - acc: 0.9794 - val_loss: 0.4077 - val_acc: 0.8747\n",
      "Epoch 443/1200\n",
      "13276/13276 [==============================] - 4s 287us/step - loss: 0.0825 - acc: 0.9832 - val_loss: 0.2352 - val_acc: 0.9478\n",
      "Epoch 444/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0859 - acc: 0.9812 - val_loss: 0.2654 - val_acc: 0.9390\n",
      "Epoch 445/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0819 - acc: 0.9834 - val_loss: 0.2434 - val_acc: 0.9492\n",
      "Epoch 446/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0862 - acc: 0.9808 - val_loss: 0.2503 - val_acc: 0.9424\n",
      "Epoch 447/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0858 - acc: 0.9814 - val_loss: 0.2436 - val_acc: 0.9485\n",
      "Epoch 448/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0899 - acc: 0.9809 - val_loss: 0.3130 - val_acc: 0.9241\n",
      "Epoch 449/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0876 - acc: 0.9820 - val_loss: 0.2454 - val_acc: 0.9431\n",
      "Epoch 450/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0884 - acc: 0.9803 - val_loss: 0.2490 - val_acc: 0.9424\n",
      "Epoch 451/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0840 - acc: 0.9823 - val_loss: 0.2441 - val_acc: 0.9472\n",
      "Epoch 452/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0856 - acc: 0.9819 - val_loss: 0.2326 - val_acc: 0.9458\n",
      "Epoch 453/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0856 - acc: 0.9828 - val_loss: 0.2380 - val_acc: 0.9485\n",
      "Epoch 454/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.0837 - acc: 0.9826 - val_loss: 0.2363 - val_acc: 0.9465\n",
      "Epoch 455/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0870 - acc: 0.9821 - val_loss: 0.2433 - val_acc: 0.9451\n",
      "Epoch 456/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0891 - acc: 0.9809 - val_loss: 0.2325 - val_acc: 0.9485\n",
      "Epoch 457/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0859 - acc: 0.9821 - val_loss: 0.2427 - val_acc: 0.9444\n",
      "Epoch 458/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0852 - acc: 0.9824 - val_loss: 0.2407 - val_acc: 0.9485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0858 - acc: 0.9812 - val_loss: 0.2867 - val_acc: 0.9370\n",
      "Epoch 460/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0895 - acc: 0.9795 - val_loss: 0.2494 - val_acc: 0.9438\n",
      "Epoch 461/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0842 - acc: 0.9821 - val_loss: 0.2534 - val_acc: 0.9458\n",
      "Epoch 462/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0830 - acc: 0.9829 - val_loss: 0.2432 - val_acc: 0.9492\n",
      "Epoch 463/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0817 - acc: 0.9840 - val_loss: 0.2185 - val_acc: 0.9539\n",
      "Epoch 464/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0885 - acc: 0.9809 - val_loss: 0.2367 - val_acc: 0.9519\n",
      "Epoch 465/1200\n",
      "13276/13276 [==============================] - 4s 310us/step - loss: 0.0817 - acc: 0.9831 - val_loss: 0.2299 - val_acc: 0.9505\n",
      "Epoch 466/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0841 - acc: 0.9824 - val_loss: 0.2237 - val_acc: 0.9499\n",
      "Epoch 467/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0862 - acc: 0.9820 - val_loss: 0.2465 - val_acc: 0.9404\n",
      "Epoch 468/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0839 - acc: 0.9832 - val_loss: 0.2594 - val_acc: 0.9404\n",
      "Epoch 469/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0859 - acc: 0.9818 - val_loss: 0.2829 - val_acc: 0.9350\n",
      "Epoch 470/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0814 - acc: 0.9833 - val_loss: 0.2525 - val_acc: 0.9431\n",
      "Epoch 471/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0845 - acc: 0.9830 - val_loss: 0.2858 - val_acc: 0.9302\n",
      "Epoch 472/1200\n",
      "13276/13276 [==============================] - 4s 308us/step - loss: 0.0860 - acc: 0.9812 - val_loss: 0.3158 - val_acc: 0.9207\n",
      "Epoch 473/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0895 - acc: 0.9803 - val_loss: 0.2626 - val_acc: 0.9404\n",
      "Epoch 474/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0854 - acc: 0.9818 - val_loss: 0.2200 - val_acc: 0.9560\n",
      "Epoch 475/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0873 - acc: 0.9818 - val_loss: 0.3667 - val_acc: 0.8970\n",
      "Epoch 476/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0839 - acc: 0.9818 - val_loss: 0.2402 - val_acc: 0.9465\n",
      "Epoch 477/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0839 - acc: 0.9827 - val_loss: 0.2366 - val_acc: 0.9485\n",
      "Epoch 478/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0862 - acc: 0.9815 - val_loss: 0.2398 - val_acc: 0.9472\n",
      "Epoch 479/1200\n",
      "13276/13276 [==============================] - 4s 288us/step - loss: 0.0871 - acc: 0.9809 - val_loss: 0.2804 - val_acc: 0.9343\n",
      "Epoch 480/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0868 - acc: 0.9817 - val_loss: 0.2380 - val_acc: 0.9458\n",
      "Epoch 481/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0814 - acc: 0.9837 - val_loss: 0.2522 - val_acc: 0.9451\n",
      "Epoch 482/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.0826 - acc: 0.9836 - val_loss: 0.2410 - val_acc: 0.9458\n",
      "Epoch 483/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0868 - acc: 0.9817 - val_loss: 0.2161 - val_acc: 0.9560\n",
      "Epoch 484/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0874 - acc: 0.9815 - val_loss: 0.2426 - val_acc: 0.9424\n",
      "Epoch 485/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0846 - acc: 0.9837 - val_loss: 0.2533 - val_acc: 0.9451\n",
      "Epoch 486/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0863 - acc: 0.9820 - val_loss: 0.3041 - val_acc: 0.9275\n",
      "Epoch 487/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0840 - acc: 0.9834 - val_loss: 0.2217 - val_acc: 0.9512\n",
      "Epoch 488/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0862 - acc: 0.9811 - val_loss: 0.2389 - val_acc: 0.9492\n",
      "Epoch 489/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0877 - acc: 0.9799 - val_loss: 0.2373 - val_acc: 0.9512\n",
      "Epoch 490/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0818 - acc: 0.9829 - val_loss: 0.2132 - val_acc: 0.9560\n",
      "Epoch 491/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0879 - acc: 0.9808 - val_loss: 0.2150 - val_acc: 0.9533\n",
      "Epoch 492/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0867 - acc: 0.9802 - val_loss: 0.2401 - val_acc: 0.9499\n",
      "Epoch 493/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0806 - acc: 0.9838 - val_loss: 0.2364 - val_acc: 0.9438\n",
      "Epoch 494/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0866 - acc: 0.9824 - val_loss: 0.2371 - val_acc: 0.9458\n",
      "Epoch 495/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0846 - acc: 0.9827 - val_loss: 0.2734 - val_acc: 0.9397\n",
      "Epoch 496/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0836 - acc: 0.9824 - val_loss: 0.2268 - val_acc: 0.9519\n",
      "Epoch 497/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0846 - acc: 0.9819 - val_loss: 0.2557 - val_acc: 0.9363\n",
      "Epoch 498/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0859 - acc: 0.9820 - val_loss: 0.2519 - val_acc: 0.9431\n",
      "Epoch 499/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0831 - acc: 0.9819 - val_loss: 0.4357 - val_acc: 0.8726\n",
      "Epoch 500/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0861 - acc: 0.9814 - val_loss: 0.2924 - val_acc: 0.9282\n",
      "Epoch 501/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0838 - acc: 0.9823 - val_loss: 0.2458 - val_acc: 0.9451\n",
      "Epoch 502/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0919 - acc: 0.9792 - val_loss: 0.2363 - val_acc: 0.9519\n",
      "Epoch 503/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0844 - acc: 0.9828 - val_loss: 0.2397 - val_acc: 0.9478\n",
      "Epoch 504/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0924 - acc: 0.9787 - val_loss: 0.2490 - val_acc: 0.9458\n",
      "Epoch 505/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0810 - acc: 0.9837 - val_loss: 0.2219 - val_acc: 0.9499\n",
      "Epoch 506/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0802 - acc: 0.9842 - val_loss: 0.3539 - val_acc: 0.9031\n",
      "Epoch 507/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0906 - acc: 0.9815 - val_loss: 0.2440 - val_acc: 0.9444\n",
      "Epoch 508/1200\n",
      "13276/13276 [==============================] - 4s 298us/step - loss: 0.0832 - acc: 0.9828 - val_loss: 0.2201 - val_acc: 0.9533\n",
      "Epoch 509/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.1075 - acc: 0.9754 - val_loss: 0.2421 - val_acc: 0.9411\n",
      "Epoch 510/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0910 - acc: 0.9801 - val_loss: 0.2405 - val_acc: 0.9458\n",
      "Epoch 511/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0867 - acc: 0.9806 - val_loss: 0.4258 - val_acc: 0.8774\n",
      "Epoch 512/1200\n",
      "13276/13276 [==============================] - 5s 368us/step - loss: 0.0874 - acc: 0.9809 - val_loss: 0.2228 - val_acc: 0.9512\n",
      "Epoch 513/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0880 - acc: 0.9812 - val_loss: 0.2609 - val_acc: 0.9404\n",
      "Epoch 514/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0901 - acc: 0.9798 - val_loss: 0.2443 - val_acc: 0.9478\n",
      "Epoch 515/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0840 - acc: 0.9828 - val_loss: 0.2110 - val_acc: 0.9580\n",
      "Epoch 516/1200\n",
      "13276/13276 [==============================] - 4s 311us/step - loss: 0.0869 - acc: 0.9812 - val_loss: 0.2550 - val_acc: 0.9411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 517/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0832 - acc: 0.9827 - val_loss: 0.2724 - val_acc: 0.9383\n",
      "Epoch 518/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0835 - acc: 0.9824 - val_loss: 0.2435 - val_acc: 0.9444\n",
      "Epoch 519/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0860 - acc: 0.9825 - val_loss: 0.2509 - val_acc: 0.9390\n",
      "Epoch 520/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0850 - acc: 0.9821 - val_loss: 0.2459 - val_acc: 0.9465\n",
      "Epoch 521/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0845 - acc: 0.9822 - val_loss: 0.2253 - val_acc: 0.9519\n",
      "Epoch 522/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0839 - acc: 0.9829 - val_loss: 0.2264 - val_acc: 0.9505\n",
      "Epoch 523/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0861 - acc: 0.9812 - val_loss: 0.2348 - val_acc: 0.9444\n",
      "Epoch 524/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0878 - acc: 0.9807 - val_loss: 0.2626 - val_acc: 0.9424\n",
      "Epoch 525/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0815 - acc: 0.9825 - val_loss: 0.2197 - val_acc: 0.9512\n",
      "Epoch 526/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0856 - acc: 0.9821 - val_loss: 0.2254 - val_acc: 0.9478\n",
      "Epoch 527/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0866 - acc: 0.9815 - val_loss: 0.3334 - val_acc: 0.9146\n",
      "Epoch 528/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0841 - acc: 0.9820 - val_loss: 0.2371 - val_acc: 0.9526\n",
      "Epoch 529/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0819 - acc: 0.9837 - val_loss: 0.2384 - val_acc: 0.9465\n",
      "Epoch 530/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0877 - acc: 0.9816 - val_loss: 0.2364 - val_acc: 0.9458\n",
      "Epoch 531/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0914 - acc: 0.9806 - val_loss: 0.2355 - val_acc: 0.9424\n",
      "Epoch 532/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0873 - acc: 0.9810 - val_loss: 0.2939 - val_acc: 0.9329\n",
      "Epoch 533/1200\n",
      "13276/13276 [==============================] - 5s 367us/step - loss: 0.0821 - acc: 0.9824 - val_loss: 0.2453 - val_acc: 0.9458\n",
      "Epoch 534/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0870 - acc: 0.9812 - val_loss: 0.2640 - val_acc: 0.9383\n",
      "Epoch 535/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0830 - acc: 0.9830 - val_loss: 0.3844 - val_acc: 0.8991\n",
      "Epoch 536/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0888 - acc: 0.9806 - val_loss: 0.2369 - val_acc: 0.9411\n",
      "Epoch 537/1200\n",
      "13276/13276 [==============================] - 4s 305us/step - loss: 0.0905 - acc: 0.9806 - val_loss: 0.2385 - val_acc: 0.9451\n",
      "Epoch 538/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0845 - acc: 0.9818 - val_loss: 0.2289 - val_acc: 0.9458\n",
      "Epoch 539/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0836 - acc: 0.9831 - val_loss: 0.2598 - val_acc: 0.9431\n",
      "Epoch 540/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0863 - acc: 0.9817 - val_loss: 0.2480 - val_acc: 0.9465\n",
      "Epoch 541/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0925 - acc: 0.9792 - val_loss: 0.3582 - val_acc: 0.8991\n",
      "Epoch 542/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0886 - acc: 0.9814 - val_loss: 0.2378 - val_acc: 0.9472\n",
      "Epoch 543/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0848 - acc: 0.9826 - val_loss: 0.2943 - val_acc: 0.9282\n",
      "Epoch 544/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0844 - acc: 0.9822 - val_loss: 0.2311 - val_acc: 0.9478\n",
      "Epoch 545/1200\n",
      "13276/13276 [==============================] - 4s 287us/step - loss: 0.0824 - acc: 0.9837 - val_loss: 0.2400 - val_acc: 0.9465\n",
      "Epoch 546/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0806 - acc: 0.9840 - val_loss: 0.2105 - val_acc: 0.9533\n",
      "Epoch 547/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0854 - acc: 0.9815 - val_loss: 0.3453 - val_acc: 0.9058\n",
      "Epoch 548/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0849 - acc: 0.9815 - val_loss: 0.2735 - val_acc: 0.9268\n",
      "Epoch 549/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0935 - acc: 0.9800 - val_loss: 0.2339 - val_acc: 0.9451\n",
      "Epoch 550/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0841 - acc: 0.9821 - val_loss: 0.3090 - val_acc: 0.9173\n",
      "Epoch 551/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0869 - acc: 0.9805 - val_loss: 0.2422 - val_acc: 0.9465\n",
      "Epoch 552/1200\n",
      "13276/13276 [==============================] - 4s 307us/step - loss: 0.0845 - acc: 0.9825 - val_loss: 0.2223 - val_acc: 0.9519\n",
      "Epoch 553/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0871 - acc: 0.9807 - val_loss: 0.2341 - val_acc: 0.9492\n",
      "Epoch 554/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0862 - acc: 0.9816 - val_loss: 0.2999 - val_acc: 0.9268\n",
      "Epoch 555/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0834 - acc: 0.9817 - val_loss: 0.3187 - val_acc: 0.9173\n",
      "Epoch 556/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0862 - acc: 0.9814 - val_loss: 0.2452 - val_acc: 0.9438\n",
      "Epoch 557/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0923 - acc: 0.9799 - val_loss: 0.4855 - val_acc: 0.8686\n",
      "Epoch 558/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.0856 - acc: 0.9827 - val_loss: 0.2739 - val_acc: 0.9377\n",
      "Epoch 559/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0851 - acc: 0.9822 - val_loss: 0.3345 - val_acc: 0.9126\n",
      "Epoch 560/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0833 - acc: 0.9837 - val_loss: 0.2304 - val_acc: 0.9512\n",
      "Epoch 561/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0831 - acc: 0.9831 - val_loss: 0.2600 - val_acc: 0.9417\n",
      "Epoch 562/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0835 - acc: 0.9824 - val_loss: 0.2435 - val_acc: 0.9472\n",
      "Epoch 563/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0776 - acc: 0.9854 - val_loss: 0.2320 - val_acc: 0.9533\n",
      "Epoch 564/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0866 - acc: 0.9815 - val_loss: 0.2504 - val_acc: 0.9431\n",
      "Epoch 565/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0914 - acc: 0.9796 - val_loss: 0.4479 - val_acc: 0.8726\n",
      "Epoch 566/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.1333 - acc: 0.9644 - val_loss: 0.2342 - val_acc: 0.9438\n",
      "Epoch 567/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0819 - acc: 0.9827 - val_loss: 0.2328 - val_acc: 0.9444\n",
      "Epoch 568/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0835 - acc: 0.9818 - val_loss: 0.2333 - val_acc: 0.9533\n",
      "Epoch 569/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0803 - acc: 0.9825 - val_loss: 0.2299 - val_acc: 0.9526\n",
      "Epoch 570/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0837 - acc: 0.9818 - val_loss: 0.2436 - val_acc: 0.9485\n",
      "Epoch 571/1200\n",
      "13276/13276 [==============================] - 5s 361us/step - loss: 0.0824 - acc: 0.9816 - val_loss: 0.4011 - val_acc: 0.8869\n",
      "Epoch 572/1200\n",
      "13276/13276 [==============================] - 4s 285us/step - loss: 0.0845 - acc: 0.9824 - val_loss: 0.2510 - val_acc: 0.9424\n",
      "Epoch 573/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0897 - acc: 0.9796 - val_loss: 0.2427 - val_acc: 0.9438\n",
      "Epoch 574/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0888 - acc: 0.9807 - val_loss: 0.2510 - val_acc: 0.9417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 575/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0880 - acc: 0.9812 - val_loss: 0.2278 - val_acc: 0.9478\n",
      "Epoch 576/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0828 - acc: 0.9833 - val_loss: 0.2799 - val_acc: 0.9390\n",
      "Epoch 577/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0869 - acc: 0.9807 - val_loss: 0.2374 - val_acc: 0.9485\n",
      "Epoch 578/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0816 - acc: 0.9837 - val_loss: 0.2210 - val_acc: 0.9512\n",
      "Epoch 579/1200\n",
      "13276/13276 [==============================] - 4s 299us/step - loss: 0.0804 - acc: 0.9828 - val_loss: 0.2317 - val_acc: 0.9485\n",
      "Epoch 580/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0844 - acc: 0.9827 - val_loss: 0.2336 - val_acc: 0.9492\n",
      "Epoch 581/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0839 - acc: 0.9827 - val_loss: 0.2317 - val_acc: 0.9512\n",
      "Epoch 582/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0869 - acc: 0.9812 - val_loss: 0.2250 - val_acc: 0.9519\n",
      "Epoch 583/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0838 - acc: 0.9823 - val_loss: 0.2304 - val_acc: 0.9478\n",
      "Epoch 584/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0832 - acc: 0.9824 - val_loss: 0.2287 - val_acc: 0.9505\n",
      "Epoch 585/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0869 - acc: 0.9815 - val_loss: 0.2340 - val_acc: 0.9478\n",
      "Epoch 586/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0863 - acc: 0.9812 - val_loss: 0.2302 - val_acc: 0.9512\n",
      "Epoch 587/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0931 - acc: 0.9793 - val_loss: 0.2695 - val_acc: 0.9329\n",
      "Epoch 588/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0996 - acc: 0.9779 - val_loss: 0.2465 - val_acc: 0.9431\n",
      "Epoch 589/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0854 - acc: 0.9821 - val_loss: 0.2068 - val_acc: 0.9573\n",
      "Epoch 590/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0886 - acc: 0.9805 - val_loss: 0.2201 - val_acc: 0.9553\n",
      "Epoch 591/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0858 - acc: 0.9818 - val_loss: 0.2319 - val_acc: 0.9492\n",
      "Epoch 592/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0820 - acc: 0.9839 - val_loss: 0.2585 - val_acc: 0.9390\n",
      "Epoch 593/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0852 - acc: 0.9828 - val_loss: 0.2301 - val_acc: 0.9465\n",
      "Epoch 594/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0858 - acc: 0.9814 - val_loss: 0.2258 - val_acc: 0.9499\n",
      "Epoch 595/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0852 - acc: 0.9818 - val_loss: 0.2651 - val_acc: 0.9417\n",
      "Epoch 596/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0834 - acc: 0.9833 - val_loss: 0.2521 - val_acc: 0.9458\n",
      "Epoch 597/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0860 - acc: 0.9819 - val_loss: 0.2348 - val_acc: 0.9444\n",
      "Epoch 598/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0836 - acc: 0.9832 - val_loss: 0.2644 - val_acc: 0.9424\n",
      "Epoch 599/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0812 - acc: 0.9826 - val_loss: 0.2546 - val_acc: 0.9424\n",
      "Epoch 600/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0876 - acc: 0.9815 - val_loss: 0.2800 - val_acc: 0.9322\n",
      "Epoch 601/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0897 - acc: 0.9808 - val_loss: 0.2559 - val_acc: 0.9383\n",
      "Epoch 602/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0821 - acc: 0.9829 - val_loss: 0.2577 - val_acc: 0.9411\n",
      "Epoch 603/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0864 - acc: 0.9813 - val_loss: 0.2397 - val_acc: 0.9465\n",
      "Epoch 604/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0873 - acc: 0.9818 - val_loss: 0.2537 - val_acc: 0.9458\n",
      "Epoch 605/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0887 - acc: 0.9797 - val_loss: 0.2733 - val_acc: 0.9397\n",
      "Epoch 606/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0928 - acc: 0.9783 - val_loss: 0.2484 - val_acc: 0.9438\n",
      "Epoch 607/1200\n",
      "13276/13276 [==============================] - 4s 280us/step - loss: 0.0849 - acc: 0.9825 - val_loss: 0.2367 - val_acc: 0.9465\n",
      "Epoch 608/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0829 - acc: 0.9828 - val_loss: 0.2335 - val_acc: 0.9451\n",
      "Epoch 609/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0896 - acc: 0.9799 - val_loss: 0.2607 - val_acc: 0.9417\n",
      "Epoch 610/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0831 - acc: 0.9821 - val_loss: 0.2494 - val_acc: 0.9451\n",
      "Epoch 611/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0862 - acc: 0.9812 - val_loss: 0.2249 - val_acc: 0.9539\n",
      "Epoch 612/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0883 - acc: 0.9814 - val_loss: 0.2163 - val_acc: 0.9512\n",
      "Epoch 613/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0854 - acc: 0.9819 - val_loss: 0.2128 - val_acc: 0.9560\n",
      "Epoch 614/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0958 - acc: 0.9797 - val_loss: 0.2553 - val_acc: 0.9424\n",
      "Epoch 615/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0876 - acc: 0.9823 - val_loss: 0.2882 - val_acc: 0.9275\n",
      "Epoch 616/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0854 - acc: 0.9821 - val_loss: 0.2985 - val_acc: 0.9282\n",
      "Epoch 617/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0873 - acc: 0.9815 - val_loss: 0.2387 - val_acc: 0.9492\n",
      "Epoch 618/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0857 - acc: 0.9818 - val_loss: 0.3579 - val_acc: 0.9038\n",
      "Epoch 619/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0888 - acc: 0.9803 - val_loss: 0.2405 - val_acc: 0.9485\n",
      "Epoch 620/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0897 - acc: 0.9798 - val_loss: 0.2197 - val_acc: 0.9546\n",
      "Epoch 621/1200\n",
      "13276/13276 [==============================] - 4s 302us/step - loss: 0.0833 - acc: 0.9822 - val_loss: 0.2341 - val_acc: 0.9485\n",
      "Epoch 622/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0854 - acc: 0.9827 - val_loss: 0.2338 - val_acc: 0.9505\n",
      "Epoch 623/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0836 - acc: 0.9828 - val_loss: 0.2558 - val_acc: 0.9404\n",
      "Epoch 624/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.0868 - acc: 0.9815 - val_loss: 0.2497 - val_acc: 0.9485\n",
      "Epoch 625/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0873 - acc: 0.9812 - val_loss: 0.2302 - val_acc: 0.9512\n",
      "Epoch 626/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0815 - acc: 0.9830 - val_loss: 0.2469 - val_acc: 0.9431\n",
      "Epoch 627/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0874 - acc: 0.9811 - val_loss: 0.2326 - val_acc: 0.9499\n",
      "Epoch 628/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0880 - acc: 0.9810 - val_loss: 0.2476 - val_acc: 0.9438\n",
      "Epoch 629/1200\n",
      "13276/13276 [==============================] - 4s 308us/step - loss: 0.0851 - acc: 0.9821 - val_loss: 0.2412 - val_acc: 0.9424\n",
      "Epoch 630/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0861 - acc: 0.9814 - val_loss: 0.2618 - val_acc: 0.9363\n",
      "Epoch 631/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0911 - acc: 0.9808 - val_loss: 0.2395 - val_acc: 0.9444\n",
      "Epoch 632/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0848 - acc: 0.9822 - val_loss: 0.2287 - val_acc: 0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 633/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0836 - acc: 0.9827 - val_loss: 0.2245 - val_acc: 0.9512\n",
      "Epoch 634/1200\n",
      "13276/13276 [==============================] - 5s 376us/step - loss: 0.0838 - acc: 0.9817 - val_loss: 0.2466 - val_acc: 0.9458\n",
      "Epoch 635/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0863 - acc: 0.9822 - val_loss: 0.2110 - val_acc: 0.9580\n",
      "Epoch 636/1200\n",
      "13276/13276 [==============================] - 4s 305us/step - loss: 0.0878 - acc: 0.9809 - val_loss: 0.2669 - val_acc: 0.9397\n",
      "Epoch 637/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0855 - acc: 0.9813 - val_loss: 0.2469 - val_acc: 0.9458\n",
      "Epoch 638/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0901 - acc: 0.9803 - val_loss: 0.4395 - val_acc: 0.8808\n",
      "Epoch 639/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0844 - acc: 0.9828 - val_loss: 0.2587 - val_acc: 0.9316\n",
      "Epoch 640/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0856 - acc: 0.9818 - val_loss: 0.2304 - val_acc: 0.9431\n",
      "Epoch 641/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0833 - acc: 0.9824 - val_loss: 0.2195 - val_acc: 0.9512\n",
      "Epoch 642/1200\n",
      "13276/13276 [==============================] - 4s 297us/step - loss: 0.0876 - acc: 0.9815 - val_loss: 0.2617 - val_acc: 0.9451\n",
      "Epoch 643/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0850 - acc: 0.9824 - val_loss: 0.2569 - val_acc: 0.9390\n",
      "Epoch 644/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0849 - acc: 0.9823 - val_loss: 0.2260 - val_acc: 0.9499\n",
      "Epoch 645/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0819 - acc: 0.9828 - val_loss: 0.2999 - val_acc: 0.9289\n",
      "Epoch 646/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0894 - acc: 0.9800 - val_loss: 0.2750 - val_acc: 0.9363\n",
      "Epoch 647/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0857 - acc: 0.9818 - val_loss: 0.2421 - val_acc: 0.9472\n",
      "Epoch 648/1200\n",
      "13276/13276 [==============================] - 4s 290us/step - loss: 0.0838 - acc: 0.9823 - val_loss: 0.2348 - val_acc: 0.9451\n",
      "Epoch 649/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0828 - acc: 0.9827 - val_loss: 0.2372 - val_acc: 0.9451\n",
      "Epoch 650/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0884 - acc: 0.9812 - val_loss: 0.2324 - val_acc: 0.9472\n",
      "Epoch 651/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0834 - acc: 0.9832 - val_loss: 0.2104 - val_acc: 0.9560\n",
      "Epoch 652/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0877 - acc: 0.9807 - val_loss: 0.5711 - val_acc: 0.8299\n",
      "Epoch 653/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0858 - acc: 0.9821 - val_loss: 0.2337 - val_acc: 0.9499\n",
      "Epoch 654/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0867 - acc: 0.9813 - val_loss: 0.2379 - val_acc: 0.9472\n",
      "Epoch 655/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0830 - acc: 0.9828 - val_loss: 0.2196 - val_acc: 0.9519\n",
      "Epoch 656/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0853 - acc: 0.9827 - val_loss: 0.3092 - val_acc: 0.9282\n",
      "Epoch 657/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0867 - acc: 0.9818 - val_loss: 0.2253 - val_acc: 0.9485\n",
      "Epoch 658/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0866 - acc: 0.9819 - val_loss: 0.2741 - val_acc: 0.9397\n",
      "Epoch 659/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0849 - acc: 0.9824 - val_loss: 0.3791 - val_acc: 0.8977\n",
      "Epoch 660/1200\n",
      "13276/13276 [==============================] - 5s 363us/step - loss: 0.0866 - acc: 0.9821 - val_loss: 0.2367 - val_acc: 0.9499\n",
      "Epoch 661/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0848 - acc: 0.9821 - val_loss: 0.2251 - val_acc: 0.9533\n",
      "Epoch 662/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0837 - acc: 0.9834 - val_loss: 0.2291 - val_acc: 0.9505\n",
      "Epoch 663/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0802 - acc: 0.9840 - val_loss: 0.2314 - val_acc: 0.9499\n",
      "Epoch 664/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0841 - acc: 0.9831 - val_loss: 0.7712 - val_acc: 0.8015\n",
      "Epoch 665/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0973 - acc: 0.9793 - val_loss: 0.2365 - val_acc: 0.9478\n",
      "Epoch 666/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0846 - acc: 0.9818 - val_loss: 0.4193 - val_acc: 0.8896\n",
      "Epoch 667/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.0867 - acc: 0.9818 - val_loss: 0.2199 - val_acc: 0.9546\n",
      "Epoch 668/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0853 - acc: 0.9813 - val_loss: 0.2533 - val_acc: 0.9397\n",
      "Epoch 669/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0845 - acc: 0.9818 - val_loss: 0.2308 - val_acc: 0.9505\n",
      "Epoch 670/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0852 - acc: 0.9812 - val_loss: 0.2142 - val_acc: 0.9560\n",
      "Epoch 671/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0849 - acc: 0.9824 - val_loss: 0.2415 - val_acc: 0.9431\n",
      "Epoch 672/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0926 - acc: 0.9794 - val_loss: 0.3345 - val_acc: 0.9119\n",
      "Epoch 673/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0857 - acc: 0.9812 - val_loss: 0.2186 - val_acc: 0.9539\n",
      "Epoch 674/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0836 - acc: 0.9821 - val_loss: 0.2488 - val_acc: 0.9492\n",
      "Epoch 675/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0844 - acc: 0.9825 - val_loss: 0.2470 - val_acc: 0.9472\n",
      "Epoch 676/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0852 - acc: 0.9826 - val_loss: 0.2184 - val_acc: 0.9512\n",
      "Epoch 677/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0943 - acc: 0.9778 - val_loss: 0.2455 - val_acc: 0.9411\n",
      "Epoch 678/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0835 - acc: 0.9826 - val_loss: 0.2697 - val_acc: 0.9343\n",
      "Epoch 679/1200\n",
      "13276/13276 [==============================] - 4s 300us/step - loss: 0.0792 - acc: 0.9849 - val_loss: 0.2169 - val_acc: 0.9546\n",
      "Epoch 680/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0917 - acc: 0.9797 - val_loss: 0.2498 - val_acc: 0.9458\n",
      "Epoch 681/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0887 - acc: 0.9814 - val_loss: 0.2243 - val_acc: 0.9492\n",
      "Epoch 682/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0846 - acc: 0.9818 - val_loss: 0.2361 - val_acc: 0.9451\n",
      "Epoch 683/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0817 - acc: 0.9834 - val_loss: 0.3340 - val_acc: 0.9146\n",
      "Epoch 684/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0856 - acc: 0.9817 - val_loss: 0.2264 - val_acc: 0.9539\n",
      "Epoch 685/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0830 - acc: 0.9821 - val_loss: 0.2988 - val_acc: 0.9268\n",
      "Epoch 686/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0843 - acc: 0.9817 - val_loss: 0.3388 - val_acc: 0.9051\n",
      "Epoch 687/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0845 - acc: 0.9819 - val_loss: 0.2531 - val_acc: 0.9438\n",
      "Epoch 688/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0843 - acc: 0.9827 - val_loss: 0.2606 - val_acc: 0.9356\n",
      "Epoch 689/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0846 - acc: 0.9831 - val_loss: 0.2392 - val_acc: 0.9458\n",
      "Epoch 690/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0850 - acc: 0.9827 - val_loss: 0.2319 - val_acc: 0.9485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 691/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0853 - acc: 0.9818 - val_loss: 0.2691 - val_acc: 0.9397\n",
      "Epoch 692/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0833 - acc: 0.9833 - val_loss: 0.2387 - val_acc: 0.9451\n",
      "Epoch 693/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0820 - acc: 0.9837 - val_loss: 0.2864 - val_acc: 0.9336\n",
      "Epoch 694/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0836 - acc: 0.9821 - val_loss: 0.2192 - val_acc: 0.9512\n",
      "Epoch 695/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0864 - acc: 0.9822 - val_loss: 0.2137 - val_acc: 0.9519\n",
      "Epoch 696/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0826 - acc: 0.9830 - val_loss: 0.2126 - val_acc: 0.9546\n",
      "Epoch 697/1200\n",
      "13276/13276 [==============================] - 4s 299us/step - loss: 0.0885 - acc: 0.9811 - val_loss: 0.2502 - val_acc: 0.9411\n",
      "Epoch 698/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0862 - acc: 0.9818 - val_loss: 0.2433 - val_acc: 0.9451\n",
      "Epoch 699/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0856 - acc: 0.9812 - val_loss: 0.2351 - val_acc: 0.9492\n",
      "Epoch 700/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0903 - acc: 0.9807 - val_loss: 0.2516 - val_acc: 0.9417\n",
      "Epoch 701/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0851 - acc: 0.9821 - val_loss: 0.2810 - val_acc: 0.9329\n",
      "Epoch 702/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0848 - acc: 0.9816 - val_loss: 0.2461 - val_acc: 0.9472\n",
      "Epoch 703/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0807 - acc: 0.9841 - val_loss: 0.2475 - val_acc: 0.9451\n",
      "Epoch 704/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0827 - acc: 0.9834 - val_loss: 0.2297 - val_acc: 0.9485\n",
      "Epoch 705/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0863 - acc: 0.9813 - val_loss: 0.2852 - val_acc: 0.9356\n",
      "Epoch 706/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0894 - acc: 0.9794 - val_loss: 0.2502 - val_acc: 0.9370\n",
      "Epoch 707/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0853 - acc: 0.9812 - val_loss: 0.2468 - val_acc: 0.9404\n",
      "Epoch 708/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.1002 - acc: 0.9790 - val_loss: 0.2312 - val_acc: 0.9492\n",
      "Epoch 709/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0836 - acc: 0.9834 - val_loss: 0.2147 - val_acc: 0.9566\n",
      "Epoch 710/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0802 - acc: 0.9831 - val_loss: 0.2485 - val_acc: 0.9478\n",
      "Epoch 711/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0824 - acc: 0.9832 - val_loss: 0.3238 - val_acc: 0.9160\n",
      "Epoch 712/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0846 - acc: 0.9822 - val_loss: 0.2439 - val_acc: 0.9472\n",
      "Epoch 713/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0885 - acc: 0.9807 - val_loss: 0.2847 - val_acc: 0.9329\n",
      "Epoch 714/1200\n",
      "13276/13276 [==============================] - 4s 305us/step - loss: 0.0883 - acc: 0.9806 - val_loss: 0.2810 - val_acc: 0.9329\n",
      "Epoch 715/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0824 - acc: 0.9828 - val_loss: 0.2399 - val_acc: 0.9492\n",
      "Epoch 716/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0876 - acc: 0.9816 - val_loss: 0.2466 - val_acc: 0.9444\n",
      "Epoch 717/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0873 - acc: 0.9812 - val_loss: 0.2323 - val_acc: 0.9512\n",
      "Epoch 718/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0819 - acc: 0.9836 - val_loss: 0.3102 - val_acc: 0.9187\n",
      "Epoch 719/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0832 - acc: 0.9825 - val_loss: 0.2576 - val_acc: 0.9451\n",
      "Epoch 720/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0803 - acc: 0.9837 - val_loss: 0.2454 - val_acc: 0.9472\n",
      "Epoch 721/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0855 - acc: 0.9826 - val_loss: 0.2278 - val_acc: 0.9505\n",
      "Epoch 722/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0826 - acc: 0.9829 - val_loss: 0.2498 - val_acc: 0.9472\n",
      "Epoch 723/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0889 - acc: 0.9805 - val_loss: 0.2453 - val_acc: 0.9431\n",
      "Epoch 724/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0847 - acc: 0.9831 - val_loss: 0.2707 - val_acc: 0.9377\n",
      "Epoch 725/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0897 - acc: 0.9803 - val_loss: 0.2666 - val_acc: 0.9390\n",
      "Epoch 726/1200\n",
      "13276/13276 [==============================] - 4s 290us/step - loss: 0.0817 - acc: 0.9836 - val_loss: 0.4005 - val_acc: 0.8835\n",
      "Epoch 727/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0881 - acc: 0.9804 - val_loss: 0.2228 - val_acc: 0.9519\n",
      "Epoch 728/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0819 - acc: 0.9834 - val_loss: 0.2733 - val_acc: 0.9363\n",
      "Epoch 729/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0842 - acc: 0.9820 - val_loss: 0.2480 - val_acc: 0.9438\n",
      "Epoch 730/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0830 - acc: 0.9819 - val_loss: 0.2431 - val_acc: 0.9417\n",
      "Epoch 731/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0840 - acc: 0.9825 - val_loss: 0.2360 - val_acc: 0.9505\n",
      "Epoch 732/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0813 - acc: 0.9828 - val_loss: 0.2734 - val_acc: 0.9343\n",
      "Epoch 733/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0876 - acc: 0.9813 - val_loss: 0.2467 - val_acc: 0.9472\n",
      "Epoch 734/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0835 - acc: 0.9825 - val_loss: 0.2579 - val_acc: 0.9451\n",
      "Epoch 735/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0852 - acc: 0.9811 - val_loss: 0.2402 - val_acc: 0.9485\n",
      "Epoch 736/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0853 - acc: 0.9826 - val_loss: 0.2661 - val_acc: 0.9411\n",
      "Epoch 737/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0849 - acc: 0.9813 - val_loss: 0.2349 - val_acc: 0.9499\n",
      "Epoch 738/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0798 - acc: 0.9836 - val_loss: 0.2245 - val_acc: 0.9560\n",
      "Epoch 739/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0866 - acc: 0.9812 - val_loss: 0.2900 - val_acc: 0.9282\n",
      "Epoch 740/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0864 - acc: 0.9815 - val_loss: 0.2490 - val_acc: 0.9424\n",
      "Epoch 741/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0816 - acc: 0.9834 - val_loss: 0.2294 - val_acc: 0.9478\n",
      "Epoch 742/1200\n",
      "13276/13276 [==============================] - 5s 357us/step - loss: 0.0865 - acc: 0.9824 - val_loss: 0.2730 - val_acc: 0.9356\n",
      "Epoch 743/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0864 - acc: 0.9800 - val_loss: 0.2751 - val_acc: 0.9370\n",
      "Epoch 744/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0849 - acc: 0.9821 - val_loss: 0.2571 - val_acc: 0.9383\n",
      "Epoch 745/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0814 - acc: 0.9832 - val_loss: 0.2465 - val_acc: 0.9472\n",
      "Epoch 746/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0783 - acc: 0.9844 - val_loss: 0.2367 - val_acc: 0.9492\n",
      "Epoch 747/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0890 - acc: 0.9803 - val_loss: 0.2478 - val_acc: 0.9431\n",
      "Epoch 748/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0860 - acc: 0.9807 - val_loss: 0.2687 - val_acc: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 749/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0858 - acc: 0.9818 - val_loss: 0.2264 - val_acc: 0.9499\n",
      "Epoch 750/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0834 - acc: 0.9828 - val_loss: 0.2454 - val_acc: 0.9472\n",
      "Epoch 751/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0879 - acc: 0.9805 - val_loss: 0.2264 - val_acc: 0.9472\n",
      "Epoch 752/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0821 - acc: 0.9833 - val_loss: 0.2323 - val_acc: 0.9472\n",
      "Epoch 753/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0822 - acc: 0.9835 - val_loss: 0.2471 - val_acc: 0.9458\n",
      "Epoch 754/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0862 - acc: 0.9820 - val_loss: 0.2415 - val_acc: 0.9478\n",
      "Epoch 755/1200\n",
      "13276/13276 [==============================] - 4s 290us/step - loss: 0.0808 - acc: 0.9837 - val_loss: 0.2400 - val_acc: 0.9465\n",
      "Epoch 756/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0842 - acc: 0.9815 - val_loss: 0.4276 - val_acc: 0.8801\n",
      "Epoch 757/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0859 - acc: 0.9810 - val_loss: 0.2460 - val_acc: 0.9424\n",
      "Epoch 758/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0800 - acc: 0.9838 - val_loss: 0.2639 - val_acc: 0.9404\n",
      "Epoch 759/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0832 - acc: 0.9822 - val_loss: 0.2323 - val_acc: 0.9505\n",
      "Epoch 760/1200\n",
      "13276/13276 [==============================] - 5s 366us/step - loss: 0.0831 - acc: 0.9828 - val_loss: 0.2336 - val_acc: 0.9478\n",
      "Epoch 761/1200\n",
      "13276/13276 [==============================] - 4s 300us/step - loss: 0.0818 - acc: 0.9840 - val_loss: 0.2380 - val_acc: 0.9472\n",
      "Epoch 762/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0856 - acc: 0.9819 - val_loss: 0.2238 - val_acc: 0.9533\n",
      "Epoch 763/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0826 - acc: 0.9828 - val_loss: 0.2605 - val_acc: 0.9411\n",
      "Epoch 764/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0878 - acc: 0.9800 - val_loss: 0.2403 - val_acc: 0.9485\n",
      "Epoch 765/1200\n",
      "13276/13276 [==============================] - 5s 360us/step - loss: 0.0851 - acc: 0.9819 - val_loss: 0.2143 - val_acc: 0.9533\n",
      "Epoch 766/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0824 - acc: 0.9830 - val_loss: 0.2292 - val_acc: 0.9526\n",
      "Epoch 767/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0884 - acc: 0.9806 - val_loss: 0.2335 - val_acc: 0.9458\n",
      "Epoch 768/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0832 - acc: 0.9830 - val_loss: 0.2701 - val_acc: 0.9404\n",
      "Epoch 769/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0847 - acc: 0.9817 - val_loss: 0.2278 - val_acc: 0.9492\n",
      "Epoch 770/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0858 - acc: 0.9822 - val_loss: 0.2239 - val_acc: 0.9505\n",
      "Epoch 771/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0833 - acc: 0.9818 - val_loss: 0.2281 - val_acc: 0.9505\n",
      "Epoch 772/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0830 - acc: 0.9822 - val_loss: 0.2437 - val_acc: 0.9451\n",
      "Epoch 773/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0829 - acc: 0.9824 - val_loss: 0.2244 - val_acc: 0.9533\n",
      "Epoch 774/1200\n",
      "13276/13276 [==============================] - 4s 296us/step - loss: 0.0831 - acc: 0.9834 - val_loss: 0.2599 - val_acc: 0.9397\n",
      "Epoch 775/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0803 - acc: 0.9834 - val_loss: 0.2453 - val_acc: 0.9478\n",
      "Epoch 776/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0888 - acc: 0.9800 - val_loss: 0.2357 - val_acc: 0.9478\n",
      "Epoch 777/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0821 - acc: 0.9834 - val_loss: 0.2279 - val_acc: 0.9485\n",
      "Epoch 778/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0952 - acc: 0.9784 - val_loss: 0.2825 - val_acc: 0.9322\n",
      "Epoch 779/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0814 - acc: 0.9833 - val_loss: 0.2324 - val_acc: 0.9485\n",
      "Epoch 780/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0827 - acc: 0.9818 - val_loss: 0.2362 - val_acc: 0.9485\n",
      "Epoch 781/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0873 - acc: 0.9804 - val_loss: 0.2286 - val_acc: 0.9492\n",
      "Epoch 782/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0824 - acc: 0.9840 - val_loss: 0.2220 - val_acc: 0.9505\n",
      "Epoch 783/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0889 - acc: 0.9791 - val_loss: 0.2742 - val_acc: 0.9377\n",
      "Epoch 784/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0841 - acc: 0.9825 - val_loss: 0.2234 - val_acc: 0.9512\n",
      "Epoch 785/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0842 - acc: 0.9820 - val_loss: 0.2592 - val_acc: 0.9438\n",
      "Epoch 786/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0835 - acc: 0.9831 - val_loss: 0.2358 - val_acc: 0.9478\n",
      "Epoch 787/1200\n",
      "13276/13276 [==============================] - 4s 299us/step - loss: 0.0881 - acc: 0.9805 - val_loss: 0.2543 - val_acc: 0.9444\n",
      "Epoch 788/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0829 - acc: 0.9829 - val_loss: 0.2214 - val_acc: 0.9519\n",
      "Epoch 789/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0858 - acc: 0.9811 - val_loss: 0.2538 - val_acc: 0.9431\n",
      "Epoch 790/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0814 - acc: 0.9831 - val_loss: 0.2560 - val_acc: 0.9390\n",
      "Epoch 791/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0860 - acc: 0.9818 - val_loss: 0.2511 - val_acc: 0.9458\n",
      "Epoch 792/1200\n",
      "13276/13276 [==============================] - 5s 364us/step - loss: 0.0827 - acc: 0.9832 - val_loss: 0.2166 - val_acc: 0.9539\n",
      "Epoch 793/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0854 - acc: 0.9820 - val_loss: 0.3491 - val_acc: 0.9038\n",
      "Epoch 794/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0833 - acc: 0.9828 - val_loss: 0.2213 - val_acc: 0.9499\n",
      "Epoch 795/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0855 - acc: 0.9818 - val_loss: 0.2555 - val_acc: 0.9424\n",
      "Epoch 796/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0839 - acc: 0.9827 - val_loss: 0.3143 - val_acc: 0.9241\n",
      "Epoch 797/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0853 - acc: 0.9808 - val_loss: 0.2473 - val_acc: 0.9431\n",
      "Epoch 798/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0817 - acc: 0.9827 - val_loss: 0.2253 - val_acc: 0.9519\n",
      "Epoch 799/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0823 - acc: 0.9818 - val_loss: 0.2420 - val_acc: 0.9438\n",
      "Epoch 800/1200\n",
      "13276/13276 [==============================] - 4s 302us/step - loss: 0.0882 - acc: 0.9812 - val_loss: 0.2439 - val_acc: 0.9424\n",
      "Epoch 801/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0911 - acc: 0.9794 - val_loss: 0.2333 - val_acc: 0.9472\n",
      "Epoch 802/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0857 - acc: 0.9816 - val_loss: 0.2480 - val_acc: 0.9451\n",
      "Epoch 803/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0841 - acc: 0.9815 - val_loss: 0.2650 - val_acc: 0.9438\n",
      "Epoch 804/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0847 - acc: 0.9812 - val_loss: 0.3173 - val_acc: 0.9214\n",
      "Epoch 805/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0852 - acc: 0.9818 - val_loss: 0.2518 - val_acc: 0.9383\n",
      "Epoch 806/1200\n",
      "13276/13276 [==============================] - 4s 289us/step - loss: 0.0865 - acc: 0.9815 - val_loss: 0.2468 - val_acc: 0.9465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 807/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0892 - acc: 0.9802 - val_loss: 0.2409 - val_acc: 0.9465\n",
      "Epoch 808/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0868 - acc: 0.9810 - val_loss: 0.2337 - val_acc: 0.9451\n",
      "Epoch 809/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0858 - acc: 0.9806 - val_loss: 0.2416 - val_acc: 0.9465\n",
      "Epoch 810/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0859 - acc: 0.9812 - val_loss: 0.2253 - val_acc: 0.9519\n",
      "Epoch 811/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0881 - acc: 0.9813 - val_loss: 0.2337 - val_acc: 0.9499\n",
      "Epoch 812/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0825 - acc: 0.9832 - val_loss: 0.2235 - val_acc: 0.9512\n",
      "Epoch 813/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0864 - acc: 0.9814 - val_loss: 0.2520 - val_acc: 0.9458\n",
      "Epoch 814/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0877 - acc: 0.9809 - val_loss: 0.2308 - val_acc: 0.9505\n",
      "Epoch 815/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0831 - acc: 0.9824 - val_loss: 0.2199 - val_acc: 0.9519\n",
      "Epoch 816/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0848 - acc: 0.9831 - val_loss: 0.2401 - val_acc: 0.9431\n",
      "Epoch 817/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0806 - acc: 0.9838 - val_loss: 0.2182 - val_acc: 0.9560\n",
      "Epoch 818/1200\n",
      "13276/13276 [==============================] - 5s 364us/step - loss: 0.0876 - acc: 0.9810 - val_loss: 0.2377 - val_acc: 0.9499\n",
      "Epoch 819/1200\n",
      "13276/13276 [==============================] - 4s 309us/step - loss: 0.0847 - acc: 0.9819 - val_loss: 0.2361 - val_acc: 0.9465\n",
      "Epoch 820/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0868 - acc: 0.9815 - val_loss: 0.2343 - val_acc: 0.9485\n",
      "Epoch 821/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0803 - acc: 0.9838 - val_loss: 0.2529 - val_acc: 0.9444\n",
      "Epoch 822/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0812 - acc: 0.9830 - val_loss: 0.2831 - val_acc: 0.9356\n",
      "Epoch 823/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0870 - acc: 0.9800 - val_loss: 0.2474 - val_acc: 0.9444\n",
      "Epoch 824/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0815 - acc: 0.9840 - val_loss: 0.2337 - val_acc: 0.9478\n",
      "Epoch 825/1200\n",
      "13276/13276 [==============================] - 4s 295us/step - loss: 0.0834 - acc: 0.9823 - val_loss: 0.2648 - val_acc: 0.9431\n",
      "Epoch 826/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0854 - acc: 0.9820 - val_loss: 0.2262 - val_acc: 0.9519\n",
      "Epoch 827/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0864 - acc: 0.9808 - val_loss: 0.2672 - val_acc: 0.9370\n",
      "Epoch 828/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0817 - acc: 0.9828 - val_loss: 0.2400 - val_acc: 0.9485\n",
      "Epoch 829/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0918 - acc: 0.9794 - val_loss: 0.2392 - val_acc: 0.9431\n",
      "Epoch 830/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0842 - acc: 0.9831 - val_loss: 0.2556 - val_acc: 0.9451\n",
      "Epoch 831/1200\n",
      "13276/13276 [==============================] - 4s 290us/step - loss: 0.0805 - acc: 0.9837 - val_loss: 0.2373 - val_acc: 0.9485\n",
      "Epoch 832/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0872 - acc: 0.9809 - val_loss: 0.2625 - val_acc: 0.9404\n",
      "Epoch 833/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0859 - acc: 0.9809 - val_loss: 0.2691 - val_acc: 0.9411\n",
      "Epoch 834/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0820 - acc: 0.9831 - val_loss: 0.2345 - val_acc: 0.9485\n",
      "Epoch 835/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0818 - acc: 0.9840 - val_loss: 0.2172 - val_acc: 0.9499\n",
      "Epoch 836/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0834 - acc: 0.9827 - val_loss: 0.2241 - val_acc: 0.9499\n",
      "Epoch 837/1200\n",
      "13276/13276 [==============================] - 4s 296us/step - loss: 0.0846 - acc: 0.9826 - val_loss: 0.2440 - val_acc: 0.9492\n",
      "Epoch 838/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0846 - acc: 0.9817 - val_loss: 0.2376 - val_acc: 0.9492\n",
      "Epoch 839/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0818 - acc: 0.9835 - val_loss: 0.2388 - val_acc: 0.9478\n",
      "Epoch 840/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0821 - acc: 0.9823 - val_loss: 0.3768 - val_acc: 0.8977\n",
      "Epoch 841/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0967 - acc: 0.9801 - val_loss: 0.2271 - val_acc: 0.9499\n",
      "Epoch 842/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0842 - acc: 0.9825 - val_loss: 0.3663 - val_acc: 0.8997\n",
      "Epoch 843/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.0844 - acc: 0.9825 - val_loss: 0.2446 - val_acc: 0.9465\n",
      "Epoch 844/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0874 - acc: 0.9814 - val_loss: 0.2363 - val_acc: 0.9472\n",
      "Epoch 845/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0834 - acc: 0.9822 - val_loss: 0.2706 - val_acc: 0.9329\n",
      "Epoch 846/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0838 - acc: 0.9827 - val_loss: 0.2516 - val_acc: 0.9411\n",
      "Epoch 847/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0875 - acc: 0.9812 - val_loss: 0.2850 - val_acc: 0.9363\n",
      "Epoch 848/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0849 - acc: 0.9828 - val_loss: 0.2604 - val_acc: 0.9383\n",
      "Epoch 849/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0830 - acc: 0.9831 - val_loss: 0.2316 - val_acc: 0.9472\n",
      "Epoch 850/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0866 - acc: 0.9810 - val_loss: 0.2366 - val_acc: 0.9458\n",
      "Epoch 851/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0850 - acc: 0.9815 - val_loss: 0.2699 - val_acc: 0.9377\n",
      "Epoch 852/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0885 - acc: 0.9813 - val_loss: 0.2333 - val_acc: 0.9478\n",
      "Epoch 853/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0900 - acc: 0.9808 - val_loss: 0.2661 - val_acc: 0.9377\n",
      "Epoch 854/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0839 - acc: 0.9821 - val_loss: 0.2753 - val_acc: 0.9356\n",
      "Epoch 855/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0841 - acc: 0.9823 - val_loss: 0.2142 - val_acc: 0.9539\n",
      "Epoch 856/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0911 - acc: 0.9793 - val_loss: 0.2802 - val_acc: 0.9316\n",
      "Epoch 857/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.0895 - acc: 0.9804 - val_loss: 0.2358 - val_acc: 0.9472\n",
      "Epoch 858/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.0809 - acc: 0.9840 - val_loss: 0.2281 - val_acc: 0.9505\n",
      "Epoch 859/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0875 - acc: 0.9817 - val_loss: 0.2402 - val_acc: 0.9438\n",
      "Epoch 860/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0825 - acc: 0.9837 - val_loss: 0.2487 - val_acc: 0.9472\n",
      "Epoch 861/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0881 - acc: 0.9809 - val_loss: 0.2358 - val_acc: 0.9458\n",
      "Epoch 862/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0851 - acc: 0.9822 - val_loss: 0.2759 - val_acc: 0.9356\n",
      "Epoch 863/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0787 - acc: 0.9831 - val_loss: 0.2679 - val_acc: 0.9377\n",
      "Epoch 864/1200\n",
      "13276/13276 [==============================] - 4s 309us/step - loss: 0.0923 - acc: 0.9785 - val_loss: 0.2561 - val_acc: 0.9424\n",
      "Epoch 865/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0862 - acc: 0.9820 - val_loss: 0.2303 - val_acc: 0.9492\n",
      "Epoch 866/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0878 - acc: 0.9812 - val_loss: 0.2375 - val_acc: 0.9485\n",
      "Epoch 867/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0865 - acc: 0.9822 - val_loss: 0.2513 - val_acc: 0.9444\n",
      "Epoch 868/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0830 - acc: 0.9834 - val_loss: 0.2633 - val_acc: 0.9411\n",
      "Epoch 869/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0877 - acc: 0.9807 - val_loss: 0.2523 - val_acc: 0.9424\n",
      "Epoch 870/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0837 - acc: 0.9826 - val_loss: 0.2250 - val_acc: 0.9499\n",
      "Epoch 871/1200\n",
      "13276/13276 [==============================] - 4s 280us/step - loss: 0.0939 - acc: 0.9784 - val_loss: 0.2587 - val_acc: 0.9377\n",
      "Epoch 872/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0814 - acc: 0.9840 - val_loss: 0.2788 - val_acc: 0.9316\n",
      "Epoch 873/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0800 - acc: 0.9840 - val_loss: 0.2439 - val_acc: 0.9472\n",
      "Epoch 874/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0825 - acc: 0.9826 - val_loss: 0.2211 - val_acc: 0.9560\n",
      "Epoch 875/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0805 - acc: 0.9835 - val_loss: 0.2497 - val_acc: 0.9417\n",
      "Epoch 876/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0843 - acc: 0.9822 - val_loss: 0.2545 - val_acc: 0.9431\n",
      "Epoch 877/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0852 - acc: 0.9820 - val_loss: 0.2359 - val_acc: 0.9492\n",
      "Epoch 878/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0778 - acc: 0.9843 - val_loss: 0.2510 - val_acc: 0.9485\n",
      "Epoch 879/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0840 - acc: 0.9822 - val_loss: 0.2751 - val_acc: 0.9390\n",
      "Epoch 880/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0865 - acc: 0.9815 - val_loss: 0.2551 - val_acc: 0.9444\n",
      "Epoch 881/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0804 - acc: 0.9838 - val_loss: 0.2927 - val_acc: 0.9350\n",
      "Epoch 882/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0825 - acc: 0.9827 - val_loss: 0.2392 - val_acc: 0.9472\n",
      "Epoch 883/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0846 - acc: 0.9824 - val_loss: 0.2432 - val_acc: 0.9485\n",
      "Epoch 884/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0801 - acc: 0.9837 - val_loss: 0.2455 - val_acc: 0.9458\n",
      "Epoch 885/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0860 - acc: 0.9813 - val_loss: 0.2426 - val_acc: 0.9485\n",
      "Epoch 886/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0880 - acc: 0.9810 - val_loss: 0.2343 - val_acc: 0.9499\n",
      "Epoch 887/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0819 - acc: 0.9826 - val_loss: 0.2298 - val_acc: 0.9512\n",
      "Epoch 888/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0947 - acc: 0.9783 - val_loss: 0.2337 - val_acc: 0.9424\n",
      "Epoch 889/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0908 - acc: 0.9796 - val_loss: 0.2588 - val_acc: 0.9390\n",
      "Epoch 890/1200\n",
      "13276/13276 [==============================] - 4s 282us/step - loss: 0.0845 - acc: 0.9820 - val_loss: 0.2673 - val_acc: 0.9350\n",
      "Epoch 891/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0859 - acc: 0.9823 - val_loss: 0.2497 - val_acc: 0.9417\n",
      "Epoch 892/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0894 - acc: 0.9816 - val_loss: 0.2794 - val_acc: 0.9363\n",
      "Epoch 893/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0878 - acc: 0.9815 - val_loss: 0.2559 - val_acc: 0.9417\n",
      "Epoch 894/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0877 - acc: 0.9797 - val_loss: 0.2854 - val_acc: 0.9146\n",
      "Epoch 895/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0870 - acc: 0.9807 - val_loss: 0.2566 - val_acc: 0.9424\n",
      "Epoch 896/1200\n",
      "13276/13276 [==============================] - 4s 283us/step - loss: 0.0863 - acc: 0.9813 - val_loss: 0.2422 - val_acc: 0.9465\n",
      "Epoch 897/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0848 - acc: 0.9812 - val_loss: 0.4356 - val_acc: 0.8801\n",
      "Epoch 898/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.1017 - acc: 0.9760 - val_loss: 0.2896 - val_acc: 0.9255\n",
      "Epoch 899/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0812 - acc: 0.9832 - val_loss: 0.2552 - val_acc: 0.9444\n",
      "Epoch 900/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0894 - acc: 0.9808 - val_loss: 0.2423 - val_acc: 0.9465\n",
      "Epoch 901/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0867 - acc: 0.9813 - val_loss: 0.2786 - val_acc: 0.9377\n",
      "Epoch 902/1200\n",
      "13276/13276 [==============================] - 5s 368us/step - loss: 0.0850 - acc: 0.9824 - val_loss: 0.2431 - val_acc: 0.9444\n",
      "Epoch 903/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0841 - acc: 0.9836 - val_loss: 0.4413 - val_acc: 0.8699\n",
      "Epoch 904/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.0879 - acc: 0.9806 - val_loss: 0.2493 - val_acc: 0.9431\n",
      "Epoch 905/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0847 - acc: 0.9824 - val_loss: 0.4910 - val_acc: 0.8523\n",
      "Epoch 906/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0855 - acc: 0.9823 - val_loss: 0.2436 - val_acc: 0.9438\n",
      "Epoch 907/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0879 - acc: 0.9803 - val_loss: 0.2256 - val_acc: 0.9485\n",
      "Epoch 908/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0855 - acc: 0.9817 - val_loss: 0.2952 - val_acc: 0.9268\n",
      "Epoch 909/1200\n",
      "13276/13276 [==============================] - 4s 298us/step - loss: 0.0854 - acc: 0.9812 - val_loss: 0.2280 - val_acc: 0.9472\n",
      "Epoch 910/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0834 - acc: 0.9816 - val_loss: 0.2261 - val_acc: 0.9492\n",
      "Epoch 911/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0833 - acc: 0.9828 - val_loss: 0.2458 - val_acc: 0.9478\n",
      "Epoch 912/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0853 - acc: 0.9815 - val_loss: 0.2422 - val_acc: 0.9485\n",
      "Epoch 913/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0858 - acc: 0.9812 - val_loss: 0.2813 - val_acc: 0.9383\n",
      "Epoch 914/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0846 - acc: 0.9828 - val_loss: 0.2316 - val_acc: 0.9499\n",
      "Epoch 915/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0852 - acc: 0.9811 - val_loss: 0.2295 - val_acc: 0.9499\n",
      "Epoch 916/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0837 - acc: 0.9813 - val_loss: 0.2357 - val_acc: 0.9485\n",
      "Epoch 917/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0842 - acc: 0.9813 - val_loss: 0.2042 - val_acc: 0.9600\n",
      "Epoch 918/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0888 - acc: 0.9800 - val_loss: 0.2129 - val_acc: 0.9566\n",
      "Epoch 919/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0836 - acc: 0.9815 - val_loss: 0.2246 - val_acc: 0.9526\n",
      "Epoch 920/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0776 - acc: 0.9851 - val_loss: 0.2505 - val_acc: 0.9444\n",
      "Epoch 921/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0870 - acc: 0.9815 - val_loss: 0.2278 - val_acc: 0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 922/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0874 - acc: 0.9811 - val_loss: 0.2293 - val_acc: 0.9499\n",
      "Epoch 923/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0859 - acc: 0.9816 - val_loss: 0.2758 - val_acc: 0.9370\n",
      "Epoch 924/1200\n",
      "13276/13276 [==============================] - 4s 298us/step - loss: 0.0905 - acc: 0.9796 - val_loss: 0.2300 - val_acc: 0.9499\n",
      "Epoch 925/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0835 - acc: 0.9832 - val_loss: 0.2210 - val_acc: 0.9492\n",
      "Epoch 926/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0830 - acc: 0.9821 - val_loss: 0.2310 - val_acc: 0.9499\n",
      "Epoch 927/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0843 - acc: 0.9821 - val_loss: 0.2248 - val_acc: 0.9492\n",
      "Epoch 928/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0837 - acc: 0.9831 - val_loss: 0.2168 - val_acc: 0.9512\n",
      "Epoch 929/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0849 - acc: 0.9830 - val_loss: 0.2220 - val_acc: 0.9499\n",
      "Epoch 930/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0882 - acc: 0.9803 - val_loss: 0.2300 - val_acc: 0.9499\n",
      "Epoch 931/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0847 - acc: 0.9825 - val_loss: 0.2325 - val_acc: 0.9472\n",
      "Epoch 932/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0857 - acc: 0.9819 - val_loss: 0.2564 - val_acc: 0.9350\n",
      "Epoch 933/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0839 - acc: 0.9817 - val_loss: 0.2297 - val_acc: 0.9492\n",
      "Epoch 934/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0869 - acc: 0.9809 - val_loss: 0.2748 - val_acc: 0.9370\n",
      "Epoch 935/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0814 - acc: 0.9833 - val_loss: 0.2289 - val_acc: 0.9505\n",
      "Epoch 936/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0834 - acc: 0.9825 - val_loss: 0.2302 - val_acc: 0.9465\n",
      "Epoch 937/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0851 - acc: 0.9821 - val_loss: 0.2471 - val_acc: 0.9472\n",
      "Epoch 938/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0830 - acc: 0.9815 - val_loss: 0.2445 - val_acc: 0.9492\n",
      "Epoch 939/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0863 - acc: 0.9806 - val_loss: 0.2319 - val_acc: 0.9485\n",
      "Epoch 940/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0819 - acc: 0.9833 - val_loss: 0.2351 - val_acc: 0.9478\n",
      "Epoch 941/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0875 - acc: 0.9813 - val_loss: 0.2458 - val_acc: 0.9472\n",
      "Epoch 942/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0832 - acc: 0.9815 - val_loss: 0.2377 - val_acc: 0.9438\n",
      "Epoch 943/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0865 - acc: 0.9816 - val_loss: 0.2319 - val_acc: 0.9519\n",
      "Epoch 944/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0853 - acc: 0.9821 - val_loss: 0.2780 - val_acc: 0.9343\n",
      "Epoch 945/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0836 - acc: 0.9822 - val_loss: 0.2813 - val_acc: 0.9370\n",
      "Epoch 946/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0833 - acc: 0.9822 - val_loss: 0.2574 - val_acc: 0.9424\n",
      "Epoch 947/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0831 - acc: 0.9827 - val_loss: 0.2483 - val_acc: 0.9465\n",
      "Epoch 948/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0897 - acc: 0.9803 - val_loss: 0.2440 - val_acc: 0.9465\n",
      "Epoch 949/1200\n",
      "13276/13276 [==============================] - 5s 348us/step - loss: 0.0807 - acc: 0.9838 - val_loss: 0.3503 - val_acc: 0.9065\n",
      "Epoch 950/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0843 - acc: 0.9830 - val_loss: 0.2242 - val_acc: 0.9519\n",
      "Epoch 951/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0829 - acc: 0.9818 - val_loss: 0.2626 - val_acc: 0.9390\n",
      "Epoch 952/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0836 - acc: 0.9826 - val_loss: 0.2441 - val_acc: 0.9465\n",
      "Epoch 953/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0866 - acc: 0.9817 - val_loss: 0.4771 - val_acc: 0.8570\n",
      "Epoch 954/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0853 - acc: 0.9814 - val_loss: 0.2475 - val_acc: 0.9451\n",
      "Epoch 955/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0829 - acc: 0.9818 - val_loss: 0.2413 - val_acc: 0.9444\n",
      "Epoch 956/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0827 - acc: 0.9824 - val_loss: 0.2364 - val_acc: 0.9458\n",
      "Epoch 957/1200\n",
      "13276/13276 [==============================] - 5s 365us/step - loss: 0.0849 - acc: 0.9815 - val_loss: 0.2286 - val_acc: 0.9499\n",
      "Epoch 958/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0806 - acc: 0.9838 - val_loss: 0.2426 - val_acc: 0.9499\n",
      "Epoch 959/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0840 - acc: 0.9818 - val_loss: 0.2393 - val_acc: 0.9465\n",
      "Epoch 960/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0811 - acc: 0.9833 - val_loss: 0.2404 - val_acc: 0.9499\n",
      "Epoch 961/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0869 - acc: 0.9812 - val_loss: 0.2250 - val_acc: 0.9499\n",
      "Epoch 962/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0880 - acc: 0.9813 - val_loss: 0.2367 - val_acc: 0.9451\n",
      "Epoch 963/1200\n",
      "13276/13276 [==============================] - 5s 354us/step - loss: 0.0833 - acc: 0.9840 - val_loss: 0.2291 - val_acc: 0.9505\n",
      "Epoch 964/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0870 - acc: 0.9808 - val_loss: 0.2282 - val_acc: 0.9526\n",
      "Epoch 965/1200\n",
      "13276/13276 [==============================] - 4s 313us/step - loss: 0.0848 - acc: 0.9815 - val_loss: 0.2640 - val_acc: 0.9431\n",
      "Epoch 966/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0809 - acc: 0.9836 - val_loss: 0.2433 - val_acc: 0.9492\n",
      "Epoch 967/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0861 - acc: 0.9809 - val_loss: 0.3025 - val_acc: 0.9289\n",
      "Epoch 968/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0832 - acc: 0.9831 - val_loss: 0.2433 - val_acc: 0.9465\n",
      "Epoch 969/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0812 - acc: 0.9825 - val_loss: 0.2498 - val_acc: 0.9444\n",
      "Epoch 970/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0822 - acc: 0.9832 - val_loss: 0.2358 - val_acc: 0.9499\n",
      "Epoch 971/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0848 - acc: 0.9816 - val_loss: 0.2189 - val_acc: 0.9560\n",
      "Epoch 972/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0807 - acc: 0.9831 - val_loss: 0.2728 - val_acc: 0.9397\n",
      "Epoch 973/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0906 - acc: 0.9792 - val_loss: 0.2747 - val_acc: 0.9343\n",
      "Epoch 974/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0920 - acc: 0.9788 - val_loss: 0.2446 - val_acc: 0.9444\n",
      "Epoch 975/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0876 - acc: 0.9808 - val_loss: 0.2344 - val_acc: 0.9472\n",
      "Epoch 976/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0843 - acc: 0.9830 - val_loss: 0.2384 - val_acc: 0.9465\n",
      "Epoch 977/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0848 - acc: 0.9827 - val_loss: 0.2639 - val_acc: 0.9404\n",
      "Epoch 978/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0857 - acc: 0.9811 - val_loss: 0.3331 - val_acc: 0.9072\n",
      "Epoch 979/1200\n",
      "13276/13276 [==============================] - 4s 291us/step - loss: 0.0857 - acc: 0.9814 - val_loss: 0.2693 - val_acc: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 980/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0904 - acc: 0.9805 - val_loss: 0.2614 - val_acc: 0.9350\n",
      "Epoch 981/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0834 - acc: 0.9815 - val_loss: 0.2386 - val_acc: 0.9451\n",
      "Epoch 982/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0802 - acc: 0.9837 - val_loss: 0.2783 - val_acc: 0.9350\n",
      "Epoch 983/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0880 - acc: 0.9808 - val_loss: 0.2403 - val_acc: 0.9492\n",
      "Epoch 984/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0805 - acc: 0.9846 - val_loss: 0.2616 - val_acc: 0.9424\n",
      "Epoch 985/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0829 - acc: 0.9831 - val_loss: 0.2280 - val_acc: 0.9505\n",
      "Epoch 986/1200\n",
      "13276/13276 [==============================] - 4s 299us/step - loss: 0.0833 - acc: 0.9823 - val_loss: 0.2907 - val_acc: 0.9322\n",
      "Epoch 987/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0826 - acc: 0.9831 - val_loss: 0.2477 - val_acc: 0.9444\n",
      "Epoch 988/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0818 - acc: 0.9830 - val_loss: 0.2421 - val_acc: 0.9465\n",
      "Epoch 989/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0847 - acc: 0.9812 - val_loss: 0.2431 - val_acc: 0.9458\n",
      "Epoch 990/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0819 - acc: 0.9834 - val_loss: 0.2331 - val_acc: 0.9485\n",
      "Epoch 991/1200\n",
      "13276/13276 [==============================] - 5s 359us/step - loss: 0.0882 - acc: 0.9804 - val_loss: 0.2350 - val_acc: 0.9478\n",
      "Epoch 992/1200\n",
      "13276/13276 [==============================] - 4s 321us/step - loss: 0.0861 - acc: 0.9815 - val_loss: 0.2807 - val_acc: 0.9356\n",
      "Epoch 993/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0844 - acc: 0.9828 - val_loss: 0.2240 - val_acc: 0.9533\n",
      "Epoch 994/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0893 - acc: 0.9793 - val_loss: 0.2449 - val_acc: 0.9444\n",
      "Epoch 995/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0840 - acc: 0.9828 - val_loss: 0.2228 - val_acc: 0.9526\n",
      "Epoch 996/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0807 - acc: 0.9831 - val_loss: 0.2316 - val_acc: 0.9505\n",
      "Epoch 997/1200\n",
      "13276/13276 [==============================] - 4s 316us/step - loss: 0.0798 - acc: 0.9840 - val_loss: 0.2173 - val_acc: 0.9560\n",
      "Epoch 998/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0825 - acc: 0.9828 - val_loss: 0.7623 - val_acc: 0.8144\n",
      "Epoch 999/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0895 - acc: 0.9806 - val_loss: 0.2936 - val_acc: 0.9309\n",
      "Epoch 1000/1200\n",
      "13276/13276 [==============================] - 4s 301us/step - loss: 0.0802 - acc: 0.9834 - val_loss: 0.2512 - val_acc: 0.9465\n",
      "Epoch 1001/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0828 - acc: 0.9820 - val_loss: 0.2275 - val_acc: 0.9526\n",
      "Epoch 1002/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0864 - acc: 0.9815 - val_loss: 0.2327 - val_acc: 0.9492\n",
      "Epoch 1003/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0848 - acc: 0.9815 - val_loss: 0.2430 - val_acc: 0.9458\n",
      "Epoch 1004/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0815 - acc: 0.9835 - val_loss: 0.2321 - val_acc: 0.9485\n",
      "Epoch 1005/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0853 - acc: 0.9825 - val_loss: 0.2997 - val_acc: 0.9241\n",
      "Epoch 1006/1200\n",
      "13276/13276 [==============================] - 4s 290us/step - loss: 0.0860 - acc: 0.9821 - val_loss: 0.3343 - val_acc: 0.9106\n",
      "Epoch 1007/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0835 - acc: 0.9828 - val_loss: 0.3420 - val_acc: 0.9065\n",
      "Epoch 1008/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0808 - acc: 0.9834 - val_loss: 0.2733 - val_acc: 0.9424\n",
      "Epoch 1009/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0837 - acc: 0.9830 - val_loss: 0.3765 - val_acc: 0.8991\n",
      "Epoch 1010/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0924 - acc: 0.9779 - val_loss: 0.2439 - val_acc: 0.9451\n",
      "Epoch 1011/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0837 - acc: 0.9815 - val_loss: 0.2271 - val_acc: 0.9492\n",
      "Epoch 1012/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0836 - acc: 0.9818 - val_loss: 0.2547 - val_acc: 0.9431\n",
      "Epoch 1013/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0829 - acc: 0.9837 - val_loss: 0.2499 - val_acc: 0.9451\n",
      "Epoch 1014/1200\n",
      "13276/13276 [==============================] - 4s 288us/step - loss: 0.0910 - acc: 0.9795 - val_loss: 0.2361 - val_acc: 0.9472\n",
      "Epoch 1015/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0828 - acc: 0.9819 - val_loss: 0.2242 - val_acc: 0.9533\n",
      "Epoch 1016/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0888 - acc: 0.9797 - val_loss: 0.2377 - val_acc: 0.9478\n",
      "Epoch 1017/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0859 - acc: 0.9818 - val_loss: 0.2310 - val_acc: 0.9485\n",
      "Epoch 1018/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0823 - acc: 0.9827 - val_loss: 0.2308 - val_acc: 0.9499\n",
      "Epoch 1019/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0852 - acc: 0.9818 - val_loss: 0.2482 - val_acc: 0.9478\n",
      "Epoch 1020/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0887 - acc: 0.9809 - val_loss: 0.2785 - val_acc: 0.9377\n",
      "Epoch 1021/1200\n",
      "13276/13276 [==============================] - 4s 302us/step - loss: 0.0839 - acc: 0.9817 - val_loss: 0.2659 - val_acc: 0.9431\n",
      "Epoch 1022/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0846 - acc: 0.9815 - val_loss: 0.4018 - val_acc: 0.8848\n",
      "Epoch 1023/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0936 - acc: 0.9795 - val_loss: 0.2401 - val_acc: 0.9526\n",
      "Epoch 1024/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0805 - acc: 0.9836 - val_loss: 0.2386 - val_acc: 0.9533\n",
      "Epoch 1025/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0911 - acc: 0.9786 - val_loss: 0.2524 - val_acc: 0.9458\n",
      "Epoch 1026/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0821 - acc: 0.9843 - val_loss: 0.2382 - val_acc: 0.9533\n",
      "Epoch 1027/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0854 - acc: 0.9821 - val_loss: 0.2623 - val_acc: 0.9404\n",
      "Epoch 1028/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0855 - acc: 0.9815 - val_loss: 0.2348 - val_acc: 0.9512\n",
      "Epoch 1029/1200\n",
      "13276/13276 [==============================] - 4s 307us/step - loss: 0.0781 - acc: 0.9854 - val_loss: 0.2377 - val_acc: 0.9519\n",
      "Epoch 1030/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0894 - acc: 0.9800 - val_loss: 0.2395 - val_acc: 0.9472\n",
      "Epoch 1031/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0825 - acc: 0.9827 - val_loss: 0.2331 - val_acc: 0.9512\n",
      "Epoch 1032/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0859 - acc: 0.9816 - val_loss: 0.2488 - val_acc: 0.9465\n",
      "Epoch 1033/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0824 - acc: 0.9822 - val_loss: 0.2383 - val_acc: 0.9505\n",
      "Epoch 1034/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0860 - acc: 0.9812 - val_loss: 0.2337 - val_acc: 0.9465\n",
      "Epoch 1035/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.1007 - acc: 0.9778 - val_loss: 0.2371 - val_acc: 0.9424\n",
      "Epoch 1036/1200\n",
      "13276/13276 [==============================] - 5s 345us/step - loss: 0.0847 - acc: 0.9821 - val_loss: 0.2544 - val_acc: 0.9424\n",
      "Epoch 1037/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0862 - acc: 0.9821 - val_loss: 0.2874 - val_acc: 0.9356\n",
      "Epoch 1038/1200\n",
      "13276/13276 [==============================] - 5s 353us/step - loss: 0.0838 - acc: 0.9832 - val_loss: 0.2151 - val_acc: 0.9526\n",
      "Epoch 1039/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0843 - acc: 0.9818 - val_loss: 0.2253 - val_acc: 0.9492\n",
      "Epoch 1040/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0867 - acc: 0.9821 - val_loss: 0.2483 - val_acc: 0.9451\n",
      "Epoch 1041/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0867 - acc: 0.9818 - val_loss: 0.2305 - val_acc: 0.9512\n",
      "Epoch 1042/1200\n",
      "13276/13276 [==============================] - 4s 307us/step - loss: 0.0952 - acc: 0.9800 - val_loss: 0.2286 - val_acc: 0.9505\n",
      "Epoch 1043/1200\n",
      "13276/13276 [==============================] - 4s 307us/step - loss: 0.0842 - acc: 0.9818 - val_loss: 0.2594 - val_acc: 0.9424\n",
      "Epoch 1044/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0894 - acc: 0.9808 - val_loss: 0.2269 - val_acc: 0.9485\n",
      "Epoch 1045/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0803 - acc: 0.9841 - val_loss: 0.2254 - val_acc: 0.9526\n",
      "Epoch 1046/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0927 - acc: 0.9782 - val_loss: 0.2437 - val_acc: 0.9458\n",
      "Epoch 1047/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0851 - acc: 0.9825 - val_loss: 0.2362 - val_acc: 0.9465\n",
      "Epoch 1048/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0851 - acc: 0.9818 - val_loss: 0.2269 - val_acc: 0.9505\n",
      "Epoch 1049/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0872 - acc: 0.9818 - val_loss: 0.2998 - val_acc: 0.9248\n",
      "Epoch 1050/1200\n",
      "13276/13276 [==============================] - 4s 308us/step - loss: 0.0846 - acc: 0.9823 - val_loss: 0.2454 - val_acc: 0.9431\n",
      "Epoch 1051/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0788 - acc: 0.9840 - val_loss: 0.2366 - val_acc: 0.9492\n",
      "Epoch 1052/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0872 - acc: 0.9818 - val_loss: 0.2487 - val_acc: 0.9417\n",
      "Epoch 1053/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0824 - acc: 0.9823 - val_loss: 0.2408 - val_acc: 0.9458\n",
      "Epoch 1054/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0856 - acc: 0.9822 - val_loss: 0.2738 - val_acc: 0.9390\n",
      "Epoch 1055/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0871 - acc: 0.9824 - val_loss: 0.2479 - val_acc: 0.9424\n",
      "Epoch 1056/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0891 - acc: 0.9804 - val_loss: 0.2438 - val_acc: 0.9458\n",
      "Epoch 1057/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0846 - acc: 0.9836 - val_loss: 0.2375 - val_acc: 0.9438\n",
      "Epoch 1058/1200\n",
      "13276/13276 [==============================] - 4s 293us/step - loss: 0.0927 - acc: 0.9788 - val_loss: 0.2343 - val_acc: 0.9485\n",
      "Epoch 1059/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.0834 - acc: 0.9818 - val_loss: 0.2745 - val_acc: 0.9370\n",
      "Epoch 1060/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0830 - acc: 0.9824 - val_loss: 0.2808 - val_acc: 0.9336\n",
      "Epoch 1061/1200\n",
      "13276/13276 [==============================] - 5s 349us/step - loss: 0.0845 - acc: 0.9818 - val_loss: 0.2412 - val_acc: 0.9505\n",
      "Epoch 1062/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0844 - acc: 0.9826 - val_loss: 0.2947 - val_acc: 0.9282\n",
      "Epoch 1063/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0826 - acc: 0.9835 - val_loss: 0.2305 - val_acc: 0.9478\n",
      "Epoch 1064/1200\n",
      "13276/13276 [==============================] - 4s 308us/step - loss: 0.0873 - acc: 0.9801 - val_loss: 0.2301 - val_acc: 0.9472\n",
      "Epoch 1065/1200\n",
      "13276/13276 [==============================] - 4s 301us/step - loss: 0.0861 - acc: 0.9803 - val_loss: 0.2292 - val_acc: 0.9485\n",
      "Epoch 1066/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0805 - acc: 0.9830 - val_loss: 0.3249 - val_acc: 0.9194\n",
      "Epoch 1067/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0805 - acc: 0.9831 - val_loss: 0.2541 - val_acc: 0.9458\n",
      "Epoch 1068/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0802 - acc: 0.9838 - val_loss: 0.2340 - val_acc: 0.9492\n",
      "Epoch 1069/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0789 - acc: 0.9841 - val_loss: 0.2874 - val_acc: 0.9302\n",
      "Epoch 1070/1200\n",
      "13276/13276 [==============================] - 5s 351us/step - loss: 0.0856 - acc: 0.9808 - val_loss: 0.2479 - val_acc: 0.9451\n",
      "Epoch 1071/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0852 - acc: 0.9819 - val_loss: 0.2223 - val_acc: 0.9512\n",
      "Epoch 1072/1200\n",
      "13276/13276 [==============================] - 4s 309us/step - loss: 0.0853 - acc: 0.9824 - val_loss: 0.2368 - val_acc: 0.9478\n",
      "Epoch 1073/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0839 - acc: 0.9822 - val_loss: 0.4849 - val_acc: 0.8652\n",
      "Epoch 1074/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0842 - acc: 0.9818 - val_loss: 0.2401 - val_acc: 0.9499\n",
      "Epoch 1075/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0839 - acc: 0.9825 - val_loss: 0.2685 - val_acc: 0.9431\n",
      "Epoch 1076/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0816 - acc: 0.9837 - val_loss: 0.2402 - val_acc: 0.9444\n",
      "Epoch 1077/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0859 - acc: 0.9821 - val_loss: 0.2465 - val_acc: 0.9465\n",
      "Epoch 1078/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0840 - acc: 0.9818 - val_loss: 0.2257 - val_acc: 0.9519\n",
      "Epoch 1079/1200\n",
      "13276/13276 [==============================] - 4s 294us/step - loss: 0.0827 - acc: 0.9822 - val_loss: 0.2841 - val_acc: 0.9289\n",
      "Epoch 1080/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0845 - acc: 0.9822 - val_loss: 0.2506 - val_acc: 0.9458\n",
      "Epoch 1081/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0838 - acc: 0.9831 - val_loss: 0.2249 - val_acc: 0.9546\n",
      "Epoch 1082/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0942 - acc: 0.9785 - val_loss: 0.2763 - val_acc: 0.9356\n",
      "Epoch 1083/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0854 - acc: 0.9818 - val_loss: 0.2720 - val_acc: 0.9322\n",
      "Epoch 1084/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0813 - acc: 0.9828 - val_loss: 0.2422 - val_acc: 0.9485\n",
      "Epoch 1085/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.0854 - acc: 0.9809 - val_loss: 0.2578 - val_acc: 0.9431\n",
      "Epoch 1086/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0822 - acc: 0.9831 - val_loss: 0.2451 - val_acc: 0.9478\n",
      "Epoch 1087/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0859 - acc: 0.9811 - val_loss: 0.2921 - val_acc: 0.9268\n",
      "Epoch 1088/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0816 - acc: 0.9834 - val_loss: 0.2511 - val_acc: 0.9444\n",
      "Epoch 1089/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0832 - acc: 0.9817 - val_loss: 0.2260 - val_acc: 0.9526\n",
      "Epoch 1090/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0874 - acc: 0.9812 - val_loss: 0.2901 - val_acc: 0.9302\n",
      "Epoch 1091/1200\n",
      "13276/13276 [==============================] - 5s 356us/step - loss: 0.0815 - acc: 0.9820 - val_loss: 0.2548 - val_acc: 0.9411\n",
      "Epoch 1092/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0829 - acc: 0.9823 - val_loss: 0.2338 - val_acc: 0.9512\n",
      "Epoch 1093/1200\n",
      "13276/13276 [==============================] - 4s 313us/step - loss: 0.0819 - acc: 0.9831 - val_loss: 0.2468 - val_acc: 0.9465\n",
      "Epoch 1094/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0839 - acc: 0.9824 - val_loss: 0.2397 - val_acc: 0.9492\n",
      "Epoch 1095/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0827 - acc: 0.9823 - val_loss: 0.2541 - val_acc: 0.9458\n",
      "Epoch 1096/1200\n",
      "13276/13276 [==============================] - 4s 329us/step - loss: 0.0833 - acc: 0.9825 - val_loss: 0.3130 - val_acc: 0.9187\n",
      "Epoch 1097/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0851 - acc: 0.9819 - val_loss: 0.2330 - val_acc: 0.9438\n",
      "Epoch 1098/1200\n",
      "13276/13276 [==============================] - 5s 350us/step - loss: 0.0827 - acc: 0.9818 - val_loss: 0.2247 - val_acc: 0.9499\n",
      "Epoch 1099/1200\n",
      "13276/13276 [==============================] - 4s 306us/step - loss: 0.0815 - acc: 0.9834 - val_loss: 0.3592 - val_acc: 0.8970\n",
      "Epoch 1100/1200\n",
      "13276/13276 [==============================] - 4s 310us/step - loss: 0.0845 - acc: 0.9813 - val_loss: 0.2147 - val_acc: 0.9539\n",
      "Epoch 1101/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0855 - acc: 0.9818 - val_loss: 0.2630 - val_acc: 0.9411\n",
      "Epoch 1102/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0814 - acc: 0.9824 - val_loss: 0.2315 - val_acc: 0.9485\n",
      "Epoch 1103/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0841 - acc: 0.9822 - val_loss: 0.2591 - val_acc: 0.9431\n",
      "Epoch 1104/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0834 - acc: 0.9822 - val_loss: 0.2195 - val_acc: 0.9546\n",
      "Epoch 1105/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0880 - acc: 0.9806 - val_loss: 0.2301 - val_acc: 0.9533\n",
      "Epoch 1106/1200\n",
      "13276/13276 [==============================] - 4s 303us/step - loss: 0.0845 - acc: 0.9832 - val_loss: 0.2674 - val_acc: 0.9417\n",
      "Epoch 1107/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0840 - acc: 0.9825 - val_loss: 0.2268 - val_acc: 0.9519\n",
      "Epoch 1108/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0869 - acc: 0.9805 - val_loss: 0.2407 - val_acc: 0.9444\n",
      "Epoch 1109/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0860 - acc: 0.9804 - val_loss: 0.2843 - val_acc: 0.9336\n",
      "Epoch 1110/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0821 - acc: 0.9838 - val_loss: 0.2366 - val_acc: 0.9499\n",
      "Epoch 1111/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0856 - acc: 0.9809 - val_loss: 0.2287 - val_acc: 0.9512\n",
      "Epoch 1112/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0829 - acc: 0.9832 - val_loss: 0.2260 - val_acc: 0.9526\n",
      "Epoch 1113/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0818 - acc: 0.9828 - val_loss: 0.2209 - val_acc: 0.9526\n",
      "Epoch 1114/1200\n",
      "13276/13276 [==============================] - 4s 323us/step - loss: 0.0873 - acc: 0.9813 - val_loss: 0.2365 - val_acc: 0.9478\n",
      "Epoch 1115/1200\n",
      "13276/13276 [==============================] - 5s 343us/step - loss: 0.0794 - acc: 0.9823 - val_loss: 0.3627 - val_acc: 0.9126\n",
      "Epoch 1116/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0919 - acc: 0.9803 - val_loss: 0.2633 - val_acc: 0.9336\n",
      "Epoch 1117/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0846 - acc: 0.9816 - val_loss: 0.2251 - val_acc: 0.9505\n",
      "Epoch 1118/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0852 - acc: 0.9816 - val_loss: 0.2328 - val_acc: 0.9485\n",
      "Epoch 1119/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0898 - acc: 0.9796 - val_loss: 0.2339 - val_acc: 0.9492\n",
      "Epoch 1120/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0833 - acc: 0.9826 - val_loss: 0.2364 - val_acc: 0.9492\n",
      "Epoch 1121/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.1098 - acc: 0.9746 - val_loss: 0.2869 - val_acc: 0.9302\n",
      "Epoch 1122/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0844 - acc: 0.9831 - val_loss: 0.2450 - val_acc: 0.9478\n",
      "Epoch 1123/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0859 - acc: 0.9813 - val_loss: 0.2216 - val_acc: 0.9533\n",
      "Epoch 1124/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0869 - acc: 0.9812 - val_loss: 0.2463 - val_acc: 0.9465\n",
      "Epoch 1125/1200\n",
      "13276/13276 [==============================] - 5s 344us/step - loss: 0.0831 - acc: 0.9834 - val_loss: 0.2677 - val_acc: 0.9370\n",
      "Epoch 1126/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0886 - acc: 0.9814 - val_loss: 0.3428 - val_acc: 0.9072\n",
      "Epoch 1127/1200\n",
      "13276/13276 [==============================] - 5s 347us/step - loss: 0.0847 - acc: 0.9821 - val_loss: 0.2342 - val_acc: 0.9478\n",
      "Epoch 1128/1200\n",
      "13276/13276 [==============================] - 4s 315us/step - loss: 0.0840 - acc: 0.9823 - val_loss: 0.2709 - val_acc: 0.9397\n",
      "Epoch 1129/1200\n",
      "13276/13276 [==============================] - 4s 282us/step - loss: 0.0847 - acc: 0.9824 - val_loss: 0.2201 - val_acc: 0.9499\n",
      "Epoch 1130/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0801 - acc: 0.9843 - val_loss: 0.2752 - val_acc: 0.9316\n",
      "Epoch 1131/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0895 - acc: 0.9802 - val_loss: 0.2250 - val_acc: 0.9505\n",
      "Epoch 1132/1200\n",
      "13276/13276 [==============================] - 5s 355us/step - loss: 0.0845 - acc: 0.9818 - val_loss: 0.3319 - val_acc: 0.9160\n",
      "Epoch 1133/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0846 - acc: 0.9818 - val_loss: 0.2341 - val_acc: 0.9492\n",
      "Epoch 1134/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0861 - acc: 0.9816 - val_loss: 0.2472 - val_acc: 0.9465\n",
      "Epoch 1135/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0848 - acc: 0.9821 - val_loss: 0.2367 - val_acc: 0.9472\n",
      "Epoch 1136/1200\n",
      "13276/13276 [==============================] - 4s 290us/step - loss: 0.0861 - acc: 0.9817 - val_loss: 0.2585 - val_acc: 0.9431\n",
      "Epoch 1137/1200\n",
      "13276/13276 [==============================] - 4s 327us/step - loss: 0.0888 - acc: 0.9803 - val_loss: 0.2405 - val_acc: 0.9485\n",
      "Epoch 1138/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0874 - acc: 0.9814 - val_loss: 0.2314 - val_acc: 0.9499\n",
      "Epoch 1139/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0807 - acc: 0.9833 - val_loss: 0.2279 - val_acc: 0.9526\n",
      "Epoch 1140/1200\n",
      "13276/13276 [==============================] - 4s 320us/step - loss: 0.0903 - acc: 0.9793 - val_loss: 0.2469 - val_acc: 0.9458\n",
      "Epoch 1141/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0843 - acc: 0.9825 - val_loss: 0.2768 - val_acc: 0.9363\n",
      "Epoch 1142/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0837 - acc: 0.9831 - val_loss: 0.2765 - val_acc: 0.9356\n",
      "Epoch 1143/1200\n",
      "13276/13276 [==============================] - 4s 283us/step - loss: 0.0873 - acc: 0.9811 - val_loss: 0.2249 - val_acc: 0.9505\n",
      "Epoch 1144/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0814 - acc: 0.9834 - val_loss: 0.2605 - val_acc: 0.9363\n",
      "Epoch 1145/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0875 - acc: 0.9809 - val_loss: 0.2844 - val_acc: 0.9309\n",
      "Epoch 1146/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0816 - acc: 0.9837 - val_loss: 0.2348 - val_acc: 0.9485\n",
      "Epoch 1147/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0830 - acc: 0.9828 - val_loss: 0.3060 - val_acc: 0.9275\n",
      "Epoch 1148/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0859 - acc: 0.9821 - val_loss: 0.2325 - val_acc: 0.9505\n",
      "Epoch 1149/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0796 - acc: 0.9843 - val_loss: 0.2404 - val_acc: 0.9472\n",
      "Epoch 1150/1200\n",
      "13276/13276 [==============================] - 4s 296us/step - loss: 0.0810 - acc: 0.9832 - val_loss: 0.2264 - val_acc: 0.9505\n",
      "Epoch 1151/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0810 - acc: 0.9840 - val_loss: 0.2867 - val_acc: 0.9275\n",
      "Epoch 1152/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0900 - acc: 0.9801 - val_loss: 0.2167 - val_acc: 0.9566\n",
      "Epoch 1153/1200\n",
      "13276/13276 [==============================] - 4s 328us/step - loss: 0.0855 - acc: 0.9824 - val_loss: 0.2414 - val_acc: 0.9492\n",
      "Epoch 1154/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0883 - acc: 0.9815 - val_loss: 0.3394 - val_acc: 0.9092\n",
      "Epoch 1155/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0840 - acc: 0.9830 - val_loss: 0.2319 - val_acc: 0.9485\n",
      "Epoch 1156/1200\n",
      "13276/13276 [==============================] - 4s 339us/step - loss: 0.0812 - acc: 0.9830 - val_loss: 0.2404 - val_acc: 0.9465\n",
      "Epoch 1157/1200\n",
      "13276/13276 [==============================] - 4s 304us/step - loss: 0.0894 - acc: 0.9800 - val_loss: 0.2734 - val_acc: 0.9411\n",
      "Epoch 1158/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0848 - acc: 0.9809 - val_loss: 0.2339 - val_acc: 0.9505\n",
      "Epoch 1159/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0933 - acc: 0.9797 - val_loss: 0.2706 - val_acc: 0.9411\n",
      "Epoch 1160/1200\n",
      "13276/13276 [==============================] - 5s 340us/step - loss: 0.0824 - acc: 0.9835 - val_loss: 0.2360 - val_acc: 0.9478\n",
      "Epoch 1161/1200\n",
      "13276/13276 [==============================] - 5s 339us/step - loss: 0.0914 - acc: 0.9797 - val_loss: 0.2537 - val_acc: 0.9438\n",
      "Epoch 1162/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0847 - acc: 0.9829 - val_loss: 0.2325 - val_acc: 0.9499\n",
      "Epoch 1163/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0848 - acc: 0.9828 - val_loss: 0.2502 - val_acc: 0.9397\n",
      "Epoch 1164/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0844 - acc: 0.9816 - val_loss: 0.2801 - val_acc: 0.9289\n",
      "Epoch 1165/1200\n",
      "13276/13276 [==============================] - 4s 282us/step - loss: 0.0853 - acc: 0.9814 - val_loss: 0.2227 - val_acc: 0.9519\n",
      "Epoch 1166/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0843 - acc: 0.9825 - val_loss: 0.2399 - val_acc: 0.9451\n",
      "Epoch 1167/1200\n",
      "13276/13276 [==============================] - 4s 333us/step - loss: 0.0825 - acc: 0.9834 - val_loss: 0.2805 - val_acc: 0.9302\n",
      "Epoch 1168/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0916 - acc: 0.9795 - val_loss: 0.2341 - val_acc: 0.9465\n",
      "Epoch 1169/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.0835 - acc: 0.9828 - val_loss: 0.3219 - val_acc: 0.9133\n",
      "Epoch 1170/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0820 - acc: 0.9831 - val_loss: 0.2319 - val_acc: 0.9492\n",
      "Epoch 1171/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0832 - acc: 0.9830 - val_loss: 0.2623 - val_acc: 0.9343\n",
      "Epoch 1172/1200\n",
      "13276/13276 [==============================] - 4s 280us/step - loss: 0.0873 - acc: 0.9810 - val_loss: 0.2698 - val_acc: 0.9350\n",
      "Epoch 1173/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0943 - acc: 0.9785 - val_loss: 0.4468 - val_acc: 0.8591\n",
      "Epoch 1174/1200\n",
      "13276/13276 [==============================] - 4s 338us/step - loss: 0.0835 - acc: 0.9823 - val_loss: 0.2307 - val_acc: 0.9478\n",
      "Epoch 1175/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0855 - acc: 0.9821 - val_loss: 0.4563 - val_acc: 0.8625\n",
      "Epoch 1176/1200\n",
      "13276/13276 [==============================] - 4s 317us/step - loss: 0.0883 - acc: 0.9803 - val_loss: 0.2757 - val_acc: 0.9309\n",
      "Epoch 1177/1200\n",
      "13276/13276 [==============================] - 4s 330us/step - loss: 0.0899 - acc: 0.9801 - val_loss: 0.2503 - val_acc: 0.9444\n",
      "Epoch 1178/1200\n",
      "13276/13276 [==============================] - 4s 325us/step - loss: 0.0848 - acc: 0.9813 - val_loss: 0.2928 - val_acc: 0.9329\n",
      "Epoch 1179/1200\n",
      "13276/13276 [==============================] - 4s 311us/step - loss: 0.0809 - acc: 0.9834 - val_loss: 0.2280 - val_acc: 0.9519\n",
      "Epoch 1180/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0818 - acc: 0.9826 - val_loss: 0.2572 - val_acc: 0.9390\n",
      "Epoch 1181/1200\n",
      "13276/13276 [==============================] - 5s 341us/step - loss: 0.0808 - acc: 0.9833 - val_loss: 0.2481 - val_acc: 0.9465\n",
      "Epoch 1182/1200\n",
      "13276/13276 [==============================] - 5s 346us/step - loss: 0.0902 - acc: 0.9793 - val_loss: 0.2405 - val_acc: 0.9438\n",
      "Epoch 1183/1200\n",
      "13276/13276 [==============================] - 5s 358us/step - loss: 0.0822 - acc: 0.9832 - val_loss: 0.4086 - val_acc: 0.8923\n",
      "Epoch 1184/1200\n",
      "13276/13276 [==============================] - 4s 319us/step - loss: 0.1025 - acc: 0.9766 - val_loss: 0.2319 - val_acc: 0.9512\n",
      "Epoch 1185/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0821 - acc: 0.9826 - val_loss: 0.2239 - val_acc: 0.9546\n",
      "Epoch 1186/1200\n",
      "13276/13276 [==============================] - 4s 337us/step - loss: 0.0841 - acc: 0.9831 - val_loss: 0.2259 - val_acc: 0.9533\n",
      "Epoch 1187/1200\n",
      "13276/13276 [==============================] - 4s 309us/step - loss: 0.0837 - acc: 0.9812 - val_loss: 0.2601 - val_acc: 0.9444\n",
      "Epoch 1188/1200\n",
      "13276/13276 [==============================] - 4s 318us/step - loss: 0.0838 - acc: 0.9827 - val_loss: 0.2442 - val_acc: 0.9465\n",
      "Epoch 1189/1200\n",
      "13276/13276 [==============================] - 5s 342us/step - loss: 0.0887 - acc: 0.9802 - val_loss: 0.2878 - val_acc: 0.9356\n",
      "Epoch 1190/1200\n",
      "13276/13276 [==============================] - 4s 332us/step - loss: 0.0856 - acc: 0.9818 - val_loss: 0.2448 - val_acc: 0.9478\n",
      "Epoch 1191/1200\n",
      "13276/13276 [==============================] - 4s 322us/step - loss: 0.0856 - acc: 0.9815 - val_loss: 0.2543 - val_acc: 0.9438\n",
      "Epoch 1192/1200\n",
      "13276/13276 [==============================] - 4s 336us/step - loss: 0.0874 - acc: 0.9804 - val_loss: 0.2626 - val_acc: 0.9390\n",
      "Epoch 1193/1200\n",
      "13276/13276 [==============================] - 4s 314us/step - loss: 0.0820 - acc: 0.9834 - val_loss: 0.2272 - val_acc: 0.9512\n",
      "Epoch 1194/1200\n",
      "13276/13276 [==============================] - 4s 312us/step - loss: 0.0832 - acc: 0.9824 - val_loss: 0.2389 - val_acc: 0.9458\n",
      "Epoch 1195/1200\n",
      "13276/13276 [==============================] - 4s 324us/step - loss: 0.0844 - acc: 0.9830 - val_loss: 0.4091 - val_acc: 0.8916\n",
      "Epoch 1196/1200\n",
      "13276/13276 [==============================] - 4s 334us/step - loss: 0.0846 - acc: 0.9819 - val_loss: 0.2432 - val_acc: 0.9458\n",
      "Epoch 1197/1200\n",
      "13276/13276 [==============================] - 4s 335us/step - loss: 0.0886 - acc: 0.9802 - val_loss: 0.2710 - val_acc: 0.9397\n",
      "Epoch 1198/1200\n",
      "13276/13276 [==============================] - 4s 326us/step - loss: 0.0848 - acc: 0.9806 - val_loss: 0.2341 - val_acc: 0.9505\n",
      "Epoch 1199/1200\n",
      "13276/13276 [==============================] - 5s 352us/step - loss: 0.0804 - acc: 0.9843 - val_loss: 0.2395 - val_acc: 0.9499\n",
      "Epoch 1200/1200\n",
      "13276/13276 [==============================] - 4s 331us/step - loss: 0.0890 - acc: 0.9809 - val_loss: 0.2428 - val_acc: 0.9472\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "current_t = time.time()\n",
    "\n",
    "verbose, epochs, batch_size = 1, 1200, 15\n",
    "n_features, n_outputs = 720, 1\n",
    "# define model\n",
    "model = Sequential()\n",
    "#kernel_regularizer=regularizers.l2(0.01),\n",
    "#model.add(LSTM(500, activation='relu',return_sequences=False, input_shape=(n_samples, n_features)))\n",
    "model.add(Dense(200, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.001), input_shape=(n_features,)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',metrics=['accuracy'], optimizer='Adam')\n",
    "model.summary()\n",
    "# fit network\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=batch_size,validation_split=0.0, verbose=verbose)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model.save('tv_model.h5')  # creates a HDF5 file 'tv_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('tv_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# #acc = history.history['acc']\n",
    "# #val_acc = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "# plt.savefig('Watching_TV_train_val_loss_curve.jpg')  # saves the current figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# plt.plot(epochs, acc, 'go', label='Training accuracy')\n",
    "# plt.plot(epochs, val_acc, 'g', label='Validation accuracy')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.savefig('Watching_TV_train_val_accuracy_curve.jpg')  # saves the current figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model.predict(test_features, verbose=0)\n",
    "results = y_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results<=0.5]=0\n",
    "results[results>0.5]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = results\n",
    "y_true = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = format(accuracy_score(y_true, y_pred),'.4f')\n",
    "\n",
    "\n",
    "sensitivity = format(recall_score(y_true, y_pred,pos_label=1,average='binary'),'.4f')\n",
    "\n",
    "specificity = format(recall_score(y_true, y_pred,pos_label=0,average='binary'),'.4f')\n",
    "\n",
    "print('Accuracy : ', accuracy)   \n",
    "print('Sensitivity : ', sensitivity)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"Features_extraction complete. Time elapsed: \" + str(int(time.time()-current_t )) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save JSON file with time and label information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = pd.DataFrame(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = pd.DataFrame(columns=['Watching_TV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoFile = \"/mnt/sde/jagadish/userdata/IMG_8307.MOV\"\n",
    "cap = cv.VideoCapture(videoFile)   # capturing the video from the given path\n",
    "fps = cap.get(cv.CAP_PROP_FPS) # Getting Franme rate of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= hf.index\n",
    "l=[]\n",
    "c=0\n",
    "for i in n[:] :\n",
    "    \n",
    "    l.append(c/fps)\n",
    "    l.append(hf.iloc[i][0])\n",
    "    \n",
    "    mf = mf.append({'Watching_TV':l[:]}, ignore_index=True)\n",
    "    l=[]\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.to_json('Time_and_Label_8307.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and save \"Time vs Label\" graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pd.DataFrame(columns=['Time', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= hf.index\n",
    "c=0\n",
    "for i in n[:] :\n",
    "    \n",
    "\n",
    "    \n",
    "    pf = pf.append({'Time': c/fps, 'Label': hf.iloc[i][0]}, ignore_index=True)\n",
    "\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pf['Time']\n",
    "label1 = pf['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(time, label1, 'g')\n",
    "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.xticks(fontsize=20, fontweight='bold',rotation=90)\n",
    "plt.yticks(fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Time (seconds)',fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Label',fontsize=20, fontweight='bold')\n",
    "plt.title('Time vs Label', fontsize=20, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.savefig('Time_and_Labels_8307.pdf')  # saves the current figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = pd.DataFrame(data=results,columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of.to_csv('out22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
